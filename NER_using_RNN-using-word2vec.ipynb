{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below load dependency pkgs. and then create manually a new directory under experiments like base_model having params.json. this json file contains hyperparameter related to model, user can tweak these parameters for finding best model. and based on new name , change below model_dir path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "# dependancy packages.\n",
    "import utils\n",
    "import model.net as net\n",
    "from model.data_loader import DataLoader\n",
    "\n",
    "\n",
    "data_dir='data/small'\n",
    "model_dir='experiments/base_model'\n",
    "word2vec_dir='experiments/word2vec' # keep GoogleNews-vectors-negative300.bin inside this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Let's use CoNLL 2002 data to build a NER system\n",
    "# \n",
    "# CoNLL2002 corpus is available in NLTK. We use Spanish data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "nltk.corpus.conll2002.fileids()\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "train_sents = [val for val in train_sents if len(val)!=0]\n",
    "train_sents = train_sents[:5000]\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "test_sents = [val for val in test_sents if len(val)!=0]\n",
    "test_sents = test_sents[:1000]\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('melbourne', '(', 'australia', ')', ',', '25', 'may', '(', 'efe', ')', '.')\n",
      "[['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp'], ['Fg'], ['DA', 'NC', 'AQ', 'SP', 'NC', 'Fc', 'VMI', 'NC', 'Fc', 'VMI', 'RG', 'DA', 'NC', 'SP', 'VMN', 'NC', 'SP', 'VMN', 'SP', 'NC', 'AQ', 'AQ', 'RG', 'SP', 'DI', 'NC', 'SP', 'NC', 'PR', 'VMI', 'DA', 'NC', 'SP', 'DA', 'NC', 'AQ', 'SP', 'DA', 'NC', 'Fp'], ['DA', 'NC', 'SP', 'NC', 'AQ', 'VMI', 'NC', 'RG', 'SP', 'CS', 'DI', 'NC', 'SP', 'NC', 'AQ', 'SP', 'NC', 'SP', 'NC', 'Fpa', 'NP', 'Fpt', 'P0', 'VMS', 'AQ', 'SP', 'VMN', 'DI', 'NC', 'AQ', 'CC', 'VMN', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'CS', 'DA', 'NC', 'PR', 'PP', 'VMI', 'VMI', 'VAN', 'VMP', 'NC', 'SP', 'DA', 'VMP', 'SP', 'NC', 'SP', 'DA', 'NC', 'AQ', 'Fp']]\n",
      "('B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O')\n"
     ]
    }
   ],
   "source": [
    "tokens_word2vec_list = []\n",
    "tr_tokens_list = []\n",
    "tr_pos_list = []\n",
    "tr_labels_list = []\n",
    "ts_tokens_list = []\n",
    "ts_pos_list = []\n",
    "ts_labels_list = []\n",
    "\n",
    "for sents in train_sents:\n",
    "    tokens = tuple(i[0].lower() for i in sents)\n",
    "    pos_tags = [i[1] for i in sents]\n",
    "    labels = tuple(i[2] for i in sents)\n",
    "    tokens_word2vec_list.append(tokens)\n",
    "    tr_tokens_list.append(tokens)\n",
    "    tr_pos_list.append(pos_tags)\n",
    "    tr_labels_list.append(labels)\n",
    "\n",
    "for sents in test_sents:\n",
    "    tokens = tuple(i[0].lower() for i in sents)\n",
    "    pos_tags = [i[1] for i in sents]\n",
    "    labels = tuple(i[2] for i in sents)\n",
    "    tokens_word2vec_list.append(tokens)\n",
    "    ts_tokens_list.append(tokens)\n",
    "    ts_pos_list.append(pos_tags)\n",
    "    ts_labels_list.append(labels)\n",
    "\n",
    "print(tr_tokens_list[0])\n",
    "print(tr_pos_list[:4])\n",
    "print(tr_labels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(637031, 960020)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = tokens_word2vec_list#[[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "model = Word2Vec(min_count=1)\n",
    "model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03542154, -0.01396509, -0.0059225 , -0.0747686 ,  0.03587869,\n",
       "        0.05271797,  0.15769364,  0.05751861,  0.01524669, -0.00404117,\n",
       "       -0.02365159, -0.04342028,  0.05798311,  0.03871023,  0.02839377,\n",
       "       -0.02556238,  0.11380405,  0.06572264, -0.04511902,  0.00613007,\n",
       "       -0.00046499, -0.03643322,  0.07480255,  0.01360533,  0.0403607 ,\n",
       "        0.04769563,  0.10762448,  0.02940772, -0.08135953,  0.09551089,\n",
       "       -0.10398597, -0.09710502,  0.02616859, -0.05593977, -0.04623265,\n",
       "        0.07723986,  0.01155668,  0.03138071,  0.0359161 ,  0.0899602 ,\n",
       "       -0.06940252, -0.07332759, -0.0480862 , -0.02716609,  0.09468073,\n",
       "       -0.03155367,  0.10605608,  0.04205232, -0.01401343,  0.02569293,\n",
       "        0.06696339, -0.03269048,  0.03017501,  0.01148963, -0.02771042,\n",
       "        0.04406427,  0.10084617,  0.03856216,  0.00645268,  0.03385818,\n",
       "       -0.05993786,  0.04428807, -0.05095945, -0.03825443,  0.02445826,\n",
       "        0.07199966,  0.03509669, -0.03542428, -0.14442119,  0.01807352,\n",
       "       -0.02513028,  0.0785576 ,  0.05001209,  0.04182822, -0.0129995 ,\n",
       "        0.00553931,  0.09971838, -0.02932747,  0.12799339, -0.10287538,\n",
       "        0.05861495, -0.11972042,  0.02184652,  0.03213316, -0.05777372,\n",
       "        0.11734612,  0.01520264, -0.03895311, -0.11590312,  0.02378197,\n",
       "       -0.03709424, -0.08255339,  0.03924714, -0.10761283,  0.02046973,\n",
       "        0.04237706,  0.0470104 , -0.0540601 ,  0.09261784, -0.02309268])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['australia'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "- done.\n",
      "Building tag vocabulary...\n",
      "- done.\n",
      "Saving vocabularies to file...\n",
      "- done.\n",
      "Characteristics of the dataset:\n",
      "- train_size: 5000\n",
      "- dev_size: 1000\n",
      "- test_size: 1000\n",
      "- vocab_size: 19952\n",
      "- number_of_tags: 9\n",
      "- pad_word: <pad>\n",
      "- pad_tag: O\n",
      "- unk_word: UNK\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build vocabularies of words and tags from datasets\"\"\"\n",
    "\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "parser = {'min_count_word':1,'min_count_tag':1,'data_dir':'data/small'}#argparse.ArgumentParser()\n",
    "#parser.add_argument('--min_count_word', default=1, help=\"Minimum count for words in the dataset\", type=int)\n",
    "#parser.add_argument('--min_count_tag', default=1, help=\"Minimum count for tags in the dataset\", type=int)\n",
    "#parser.add_argument('--data_dir', default='data/small', help=\"Directory containing the dataset\")\n",
    "\n",
    "# Hyper parameters for the vocab\n",
    "PAD_WORD = '<pad>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = 'UNK'\n",
    "\n",
    "\n",
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
    "\n",
    "    Args:\n",
    "        vocab: (iterable object) yields token\n",
    "        txt_path: (stirng) path to vocab file\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict to json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: v for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "\n",
    "def update_vocab(tokens_list, vocab):\n",
    "    \"\"\"Update word and tag vocabulary from dataset\n",
    "\n",
    "    Args:\n",
    "        txt_path: (string) path to file, one sentence per line\n",
    "        vocab: (dict or Counter) with update method\n",
    "\n",
    "    Returns:\n",
    "        dataset_size: (int) number of elements in the dataset\n",
    "    \"\"\"\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    # Build word vocab with train and test datasets\n",
    "    print(\"Building word vocabulary...\")\n",
    "    words = Counter()\n",
    "    size_train_sentences = update_vocab(tr_tokens_list, words)\n",
    "    size_dev_sentences = update_vocab(ts_tokens_list, words)\n",
    "    size_test_sentences = update_vocab(ts_tokens_list, words)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Build tag vocab with train and test datasets\n",
    "    print(\"Building tag vocabulary...\")\n",
    "    tags = Counter()\n",
    "    size_train_tags = update_vocab(tr_labels_list, tags)\n",
    "    size_dev_tags = update_vocab(ts_labels_list, tags)\n",
    "    size_test_tags = update_vocab(ts_labels_list, tags)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Assert same number of examples in datasets\n",
    "    assert size_train_sentences == size_train_tags\n",
    "    assert size_dev_sentences == size_dev_tags\n",
    "    assert size_test_sentences == size_test_tags\n",
    "\n",
    "    # Only keep most frequent tokens\n",
    "    words = [tok for tok, count in words.items() if count >= parser['min_count_word']]\n",
    "    tags = [tok for tok, count in tags.items() if count >= parser['min_count_tag']]\n",
    "\n",
    "    # Add pad tokens\n",
    "    if PAD_WORD not in words: words.append(PAD_WORD)\n",
    "    if PAD_TAG not in tags: tags.append(PAD_TAG)\n",
    "    \n",
    "    # add word for unknown words \n",
    "    words.append(UNK_WORD)\n",
    "\n",
    "    # Save vocabularies to file\n",
    "    print(\"Saving vocabularies to file...\")\n",
    "    save_vocab_to_txt_file(words, os.path.join(parser['data_dir'], 'words.txt'))\n",
    "    save_vocab_to_txt_file(tags, os.path.join(parser['data_dir'], 'tags.txt'))\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Save datasets properties in json file\n",
    "    sizes = {\n",
    "        'train_size': size_train_sentences,\n",
    "        'dev_size': size_dev_sentences,\n",
    "        'test_size': size_test_sentences,\n",
    "        'vocab_size': len(words),\n",
    "        'number_of_tags': len(tags),\n",
    "        'pad_word': PAD_WORD,\n",
    "        'pad_tag': PAD_TAG,\n",
    "        'unk_word': UNK_WORD\n",
    "    }\n",
    "    \n",
    "    save_dict_to_json(sizes, os.path.join(parser['data_dir'], 'dataset_params.json'))\n",
    "\n",
    "    # Logging sizes\n",
    "    to_print = \"\\n\".join(\"- {}: {}\".format(k, v) for k, v in sizes.items())\n",
    "    print(\"Characteristics of the dataset:\\n{}\".format(to_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'DN', 'Fs', 'VSN', 'Fc', 'P0', 'Fit', 'DT', 'Fpt', 'VMS', 'CC', 'PN', 'VSG', 'DP', 'Faa', 'VMM', 'Fg', 'Fh', 'SP', 'Fe', 'Z', 'NC', 'Fd', 'VSP', 'DI', 'PP', 'PT', 'VAS', 'AO', 'Fp', 'Ft', 'DA', 'PR', 'VSS', 'Fat', 'VMP', 'Fia', 'AQ', 'VSI', 'PX', 'PD', 'VAP', 'VSM', 'RG', 'I', 'Fpa', 'VAI', 'VMN', 'VAN', 'VMI', 'NP', 'CS', 'Fz', 'PI', 'RN', 'VMG', 'Fx', 'DD']\n",
      "[50, 45, 50, 8, 4, 20, 21, 45, 21, 8, 29]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "NP\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(pos_list)#([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])   \n",
    "print(enc.n_values_)\n",
    "print(enc.feature_indices_)\n",
    "print(enc.transform([('NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp')]).toarray())\n",
    "'''\n",
    "from numpy import argmax\n",
    "import itertools\n",
    "\n",
    "# define input string\n",
    "data = ['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp']\n",
    "\n",
    "# define universe of possible input values\n",
    "unq_pos_list = list(itertools.chain.from_iterable(tr_pos_list+ts_pos_list))\n",
    "unq_pos_list = list(set(unq_pos_list))\n",
    "print(unq_pos_list)\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unq_pos_list))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(unq_pos_list))\n",
    "\n",
    "def onehot_encode(data,unq_pos_list,char_to_int):\n",
    "    # integer encode input data\n",
    "    integer_encoded = [char_to_int[pos] for pos in data]\n",
    "    print(integer_encoded)\n",
    "\n",
    "    # one hot encode\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "\t    letter = [0 for _ in range(len(unq_pos_list))]\n",
    "\t    letter[value] = 1\n",
    "\t    onehot_encoded.append(letter)\n",
    "    #print(onehot_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "onehot_encoded = onehot_encode(data,unq_pos_list,char_to_int)\n",
    "print(onehot_encoded)\n",
    "\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train & evaluation wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader(object):\n",
    "    def data_iterator(self, data_dir, data, params, shuffle=False):\n",
    "        \"\"\"\n",
    "        Returns a generator that yields batches data with labels. Batch size is params.batch_size. Expires after one\n",
    "        pass over the data.\n",
    "\n",
    "        Args:\n",
    "            data: (dict) contains data which has keys 'data', 'labels' and 'size'\n",
    "            params: (Params) hyperparameters of the training process.\n",
    "            shuffle: (bool) whether the data should be shuffled\n",
    "\n",
    "        Yields:\n",
    "            batch_data: (Variable) dimension batch_size x seq_len with the sentence data\n",
    "            batch_labels: (Variable) dimension batch_size x seq_len with the corresponding labels\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(230)\n",
    "            random.shuffle(order)\n",
    "\n",
    "        # loading dataset_params\n",
    "        json_path = os.path.join(data_dir, 'dataset_params.json')\n",
    "        assert os.path.isfile(json_path), \"No json file found at {}, run build_vocab.py\".format(json_path)\n",
    "        dataset_params = utils.Params(json_path)        \n",
    "\n",
    "        # loading vocab (we require this to map words to their indices)\n",
    "        vocab_path = os.path.join(data_dir, 'words.txt')\n",
    "        vocab = {}\n",
    "        with open(vocab_path) as f:\n",
    "            for i, l in enumerate(f.read().splitlines()):\n",
    "                vocab[l] = i\n",
    "\n",
    "        # setting the indices for UNKnown words and PADding symbols\n",
    "        self.unk_ind = vocab[dataset_params.unk_word]\n",
    "        self.pad_ind = vocab[dataset_params.pad_word]\n",
    "        \n",
    "\n",
    "        # one pass over data\n",
    "        for i in range((data['size']+1)//params.batch_size):\n",
    "            # fetch sentences and tags\n",
    "            batch_sentences = [data['data'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "            batch_tags = [data['labels'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "\n",
    "            # compute length of longest sentence in batch\n",
    "            batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "            # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "            # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "            batch_data = self.pad_ind*np.ones((len(batch_sentences), batch_max_len, 100))\n",
    "            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "            \n",
    "            print(\"------------------------------\")\n",
    "            print(np.array(batch_sentences).shape)\n",
    "            print(batch_data.shape)\n",
    "            print(batch_labels.shape)\n",
    "\n",
    "            # copy the data to the numpy array\n",
    "            for j in range(len(batch_sentences)):\n",
    "                cur_len = len(batch_sentences[j])\n",
    "                print(cur_len)\n",
    "                print(batch_data[j][:cur_len].shape)\n",
    "                batch_data[j][:cur_len] = batch_sentences[j]\n",
    "                batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "            # since all data are indices, we convert them to torch LongTensors\n",
    "            batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "            print(type(batch_data))\n",
    "            print(type(batch_labels))\n",
    "            batch_data = batch_data.to(dtype=torch.long)\n",
    "            batch_labels = batch_labels.to(dtype=torch.long)\n",
    "\n",
    "            # shift tensors to GPU if available\n",
    "            if params.cuda:\n",
    "                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "            # convert them to Variables to record operations in the computational graph\n",
    "            batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "            print(type(batch_data))\n",
    "            print(type(batch_labels))\n",
    "            print(batch_data.dtype)\n",
    "            print(batch_data[0].dtype)\n",
    "            print(batch_data[0])\n",
    "\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps) \n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch = next(data_iterator)\n",
    "        print(\"-------------------------\")\n",
    "        print(train_batch.shape)\n",
    "        print(labels_batch.shape)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.data.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    \n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch = next(data_iterator)\n",
    "        \n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.data.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    return metrics_mean\n",
    "\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        dl = data_loader()\n",
    "        train_data_iterator = dl.data_iterator(data_dir, train_data, params, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, metrics, params, num_steps)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_data_iterator = data_iterator(val_data, params, shuffle=False)\n",
    "        val_metrics = evaluate(model, loss_fn, val_data_iterator, metrics, params, num_steps)\n",
    "        \n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()}, \n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "            \n",
    "        # If best_eval, best_save_path        \n",
    "        if is_best:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "- done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 45, 50, 8, 4, 20, 21, 45, 21, 8, 29]\n",
      "[16]\n",
      "[31, 21, 37, 18, 21, 4, 49, 21, 4, 49, 43, 31, 21, 18, 47, 21, 18, 47, 18, 21, 37, 37, 43, 18, 24, 21, 18, 21, 32, 49, 31, 21, 18, 31, 21, 37, 18, 31, 21, 29]\n",
      "[31, 21, 18, 21, 37, 49, 21, 43, 18, 51, 24, 21, 18, 21, 37, 18, 21, 18, 21, 45, 50, 8, 5, 9, 37, 18, 47, 24, 21, 37, 10, 47, 31, 21, 18, 31, 21, 18, 31, 21, 18, 51, 31, 21, 32, 25, 49, 49, 48, 35, 21, 18, 31, 35, 18, 21, 18, 31, 21, 37, 29]\n",
      "[57, 21, 37, 49, 24, 21, 18, 21, 4, 21, 18, 31, 32, 46, 23, 35, 18, 43, 18, 24, 21, 18, 21, 4, 10, 49, 21, 18, 11, 18, 21, 10, 15, 43, 35, 10, 43, 15, 4, 21, 32, 49, 18, 21, 18, 21, 10, 21, 37, 29]\n",
      "[31, 21, 4, 20, 21, 45, 50, 8, 29]\n",
      "[16]\n",
      "[31, 21, 19, 21, 37, 19, 18, 21, 37, 18, 21, 18, 21, 49, 18, 47, 24, 20, 18, 11, 18, 31, 28, 21, 18, 57, 21, 18, 21, 18, 24, 21, 18, 20, 4, 49, 43, 37, 21, 37, 4, 37, 18, 21, 18, 21, 37, 29]\n",
      "[21, 37, 49, 18, 31, 21, 31, 21, 37, 4, 32, 49, 31, 37, 21, 18, 31, 21, 18, 21, 37, 18, 47, 21, 18, 21, 10, 21, 4, 43, 51, 21, 18, 21, 4, 10, 32, 43, 49, 47, 43, 31, 37, 21, 18, 21, 18, 21, 29]\n",
      "[31, 21, 49, 47, 31, 21, 19, 21, 19, 4, 32, 31, 37, 21, 49, 43, 43, 18, 1, 21, 18, 21, 18, 21, 4, 18, 47, 21, 18, 21, 18, 21, 4, 21, 37, 10, 21, 37, 4, 51, 31, 21, 37, 49, 43, 18, 21, 18, 24, 21, 18, 21, 4, 49, 31, 21, 18, 21, 37, 29]\n",
      "[0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epoch(s)\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "(5,)\n",
      "(5, 61, 100)\n",
      "(5, 61)\n",
      "50\n",
      "(50, 100)\n",
      "11\n",
      "(11, 100)\n",
      "61\n",
      "(61, 100)\n",
      "40\n",
      "(40, 100)\n",
      "1\n",
      "(1, 100)\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.int64\n",
      "torch.int64\n",
      "tensor([[    0,     0,     0,  ...,    -1,     1,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [19950, 19950, 19950,  ..., 19950, 19950, 19950],\n",
      "        [19950, 19950, 19950,  ..., 19950, 19950, 19950],\n",
      "        [19950, 19950, 19950,  ..., 19950, 19950, 19950]], device='cuda:0')\n",
      "-------------------------\n",
      "torch.Size([5, 61, 100])\n",
      "torch.Size([5, 61])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0b2063b8d738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training for {} epoch(s)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir,\n\u001b[0;32m---> 99\u001b[0;31m                     restore_file)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-769fe5d501e5>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir, restore_file)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mtrain_data_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Evaluate for one epoch on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-769fe5d501e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# compute model output and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/venv3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/NER-Test/model/net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# run the LSTM along the sentences of length seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# dim: batch_size x seq_len x lstm_hidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# make the Variable contiguous in memory (a PyTorch artefact)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/venv3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/venv3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "restore_file=None\n",
    "\n",
    "# Load the parameters from json file\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "    \n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "        \n",
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "    \n",
    "# load data\n",
    "#data_loader = DataLoader(data_dir, params)\n",
    "#data = data_loader.load_data(['train', 'val'], data_dir)\n",
    "#train_data = data['train']\n",
    "#val_data = data['val']\n",
    "params.vocab_size=sizes['vocab_size']\n",
    "params.number_of_tags = sizes['number_of_tags']\n",
    "\n",
    "# add word2vec\n",
    "'''\n",
    "train_data = {'data':[[],[]],'labels':[[]],'size':10}\n",
    "'''\n",
    "tr_data = None\n",
    "tr_pos = None\n",
    "ts_data = None\n",
    "ts_pos = None\n",
    "def preprocess_data(tokens_list,pos_list):\n",
    "    tr_data = []\n",
    "    for v in tokens_list[:5]:\n",
    "        tmp_tr_data = []\n",
    "        for v_in in v:\n",
    "            tmp_tr_data.append(model[v_in.lower()].tolist()) #.astype(np.float64)\n",
    "        tr_data.append(tmp_tr_data)\n",
    "    \n",
    "    tr_pos = [onehot_encode(v,unq_pos_list,char_to_int) for v in pos_list[:5]]\n",
    "    return tr_data,tr_pos\n",
    "\n",
    "tr_data,tr_pos = preprocess_data(tr_tokens_list,tr_pos_list)\n",
    "ts_data,ts_pos = preprocess_data(ts_tokens_list,ts_pos_list)\n",
    "\n",
    "\n",
    "# loading tags (we require this to map tags to their indices)\n",
    "tags_path = os.path.join(data_dir, 'tags.txt')\n",
    "tag_map = {}\n",
    "with open(tags_path) as f:\n",
    "    for i, t in enumerate(f.read().splitlines()):\n",
    "        tag_map[t] = i\n",
    "\n",
    "def preprocess_label(tag_map, labels_list):\n",
    "    fnl_labels = []\n",
    "    for label_sent in tr_labels_list:\n",
    "        tmp_label = []\n",
    "        for label in label_sent:\n",
    "            tmp_label.append(tag_map[label])\n",
    "        fnl_labels.append(tmp_label)\n",
    "    return fnl_labels\n",
    "\n",
    "tr_label_list = preprocess_label(tag_map, tr_labels_list)\n",
    "ts_label_list = preprocess_label(tag_map, ts_labels_list)\n",
    "print(tr_label_list[0])\n",
    "#print(tr_tokens_list[0])\n",
    "#print(tr_data[0])\n",
    "#print(tr_pos[0])\n",
    "#print(tr_labels_list[0])\n",
    "#print(params)\n",
    "\n",
    "train_data = {'data':tr_data,'labels':tr_label_list[:5],'size':5}\n",
    "val_data = {'data':ts_data,'labels':ts_label_list[:5],'size':5}\n",
    "\n",
    "#import sys\n",
    "#sys.exit()\n",
    "# specify the train and val dataset sizes\n",
    "params.train_size = train_data['size']\n",
    "params.val_size = val_data['size']\n",
    "\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "# Define the model and optimizer\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    \n",
    "# fetch loss function and metrics\n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir,\n",
    "                    restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "(5,)\n",
      "(5, 61, 100)\n",
      "(5, 61)\n",
      "50\n",
      "(50, 100)\n",
      "11\n",
      "(11, 100)\n",
      "61\n",
      "(61, 100)\n",
      "40\n",
      "(40, 100)\n",
      "1\n",
      "(1, 100)\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.int64\n",
      "torch.int64\n",
      "tensor([[    1,     0,     0,  ...,     0,     0,     1],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [19950, 19950, 19950,  ..., 19950, 19950, 19950],\n",
      "        [19950, 19950, 19950,  ..., 19950, 19950, 19950],\n",
      "        [19950, 19950, 19950,  ..., 19950, 19950, 19950]], device='cuda:0')\n",
      "-------------------------\n",
      "torch.Size([5, 61, 100])\n",
      "torch.Size([5, 61])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c39fe9d0e9e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir,\n\u001b[0;32m----> 2\u001b[0;31m                     restore_file)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-769fe5d501e5>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir, restore_file)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mtrain_data_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Evaluate for one epoch on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-769fe5d501e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# compute model output and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/venv3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/NER-Test/model/net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# run the LSTM along the sentences of length seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# dim: batch_size x seq_len x lstm_hidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# make the Variable contiguous in memory (a PyTorch artefact)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/venv3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/user_achyuta/venv3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir,\n",
    "                    restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Net' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-58e53c8811b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'australia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Net' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model['australia'].dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [[0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   9,\n",
       "   15,\n",
       "   1,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21],\n",
       "  [22,\n",
       "   1,\n",
       "   23,\n",
       "   24,\n",
       "   11,\n",
       "   9,\n",
       "   25,\n",
       "   26,\n",
       "   9,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   35,\n",
       "   13,\n",
       "   35,\n",
       "   40,\n",
       "   9,\n",
       "   41,\n",
       "   21,\n",
       "   35],\n",
       "  [42, 4, 18, 9, 43, 1, 44, 7, 45, 46, 11, 47, 48, 21],\n",
       "  [49, 50, 9, 51, 1, 52, 53, 54, 55, 56, 57, 58, 59, 60, 21],\n",
       "  [61,\n",
       "   8,\n",
       "   62,\n",
       "   63,\n",
       "   9,\n",
       "   64,\n",
       "   1,\n",
       "   9,\n",
       "   65,\n",
       "   66,\n",
       "   1,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   11,\n",
       "   9,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   1,\n",
       "   76,\n",
       "   21],\n",
       "  [61,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   67,\n",
       "   68,\n",
       "   81,\n",
       "   11,\n",
       "   9,\n",
       "   12,\n",
       "   25,\n",
       "   13,\n",
       "   9,\n",
       "   82,\n",
       "   83,\n",
       "   1,\n",
       "   84,\n",
       "   16,\n",
       "   17,\n",
       "   11,\n",
       "   19,\n",
       "   20,\n",
       "   21],\n",
       "  [61,\n",
       "   6,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   1,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   11,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   93,\n",
       "   96,\n",
       "   93,\n",
       "   13,\n",
       "   97,\n",
       "   21],\n",
       "  [61,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   78,\n",
       "   7,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   1,\n",
       "   105,\n",
       "   11,\n",
       "   106,\n",
       "   107,\n",
       "   63,\n",
       "   108,\n",
       "   7,\n",
       "   109,\n",
       "   7,\n",
       "   110,\n",
       "   68,\n",
       "   111,\n",
       "   1,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   21],\n",
       "  [110, 115, 116, 117, 118, 1, 9, 114, 119, 53, 120, 121, 122, 123, 21],\n",
       "  [124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   7,\n",
       "   129,\n",
       "   130,\n",
       "   7,\n",
       "   131,\n",
       "   132,\n",
       "   118,\n",
       "   1,\n",
       "   9,\n",
       "   123,\n",
       "   107,\n",
       "   93,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   21]],\n",
       " 'labels': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 6, 0, 0, 0, 2, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
       "  [0,\n",
       "   5,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   7,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 5, 0, 0, 0, 0, 0]],\n",
       " 'size': 10}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7427669902912621, 'loss': 1.8864147663116455}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "restore_file='best'\n",
    "\n",
    "# Load the parameters\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()     # use GPU is available\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "# load data\n",
    "data_loader = DataLoader(data_dir, params)\n",
    "data = data_loader.load_data(['test'], data_dir)\n",
    "test_data = data['test']\n",
    "\n",
    "# specify the test set size\n",
    "params.test_size = test_data['size']\n",
    "test_data_iterator = data_loader.data_iterator(test_data, params)\n",
    "\n",
    "# Define the model\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "    \n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Reload weights from the saved file\n",
    "utils.load_checkpoint(os.path.join(model_dir, restore_file + '.pth.tar'), model)\n",
    "\n",
    "# Evaluate\n",
    "num_steps = (params.test_size + 1) // params.batch_size\n",
    "test_metrics = evaluate(model, loss_fn, test_data_iterator, metrics, params, num_steps)\n",
    "save_path = os.path.join(model_dir, \"metrics_test_{}.json\".format(restore_file))\n",
    "utils.save_dict_to_json(test_metrics, save_path)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "\u001b[K    100% || 1.4MB 790kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/achyuta/Achyuta/venv3.6/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Collecting singledispatch (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/achyuta/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
