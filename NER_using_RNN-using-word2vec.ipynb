{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below load dependency pkgs. and then create manually a new directory under experiments like base_model having params.json. this json file contains hyperparameter related to model, user can tweak these parameters for finding best model. and based on new name , change below model_dir path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "# dependancy packages.\n",
    "import utils\n",
    "import model.net as net\n",
    "from model.data_loader import DataLoader\n",
    "\n",
    "\n",
    "data_dir='data/small'\n",
    "model_dir='experiments/base_model'\n",
    "word2vec_dir='experiments/word2vec' # keep GoogleNews-vectors-negative300.bin inside this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Let's use CoNLL 2002 data to build a NER system\n",
    "# \n",
    "# CoNLL2002 corpus is available in NLTK. We use Spanish data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "nltk.corpus.conll2002.fileids()\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "train_sents = [val for val in train_sents if len(val)!=0]\n",
    "train_sents = train_sents[:5000]\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "test_sents = [val for val in test_sents if len(val)!=0]\n",
    "test_sents = test_sents[:1000]\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.')\n",
      "[['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp'], ['Fg'], ['DA', 'NC', 'AQ', 'SP', 'NC', 'Fc', 'VMI', 'NC', 'Fc', 'VMI', 'RG', 'DA', 'NC', 'SP', 'VMN', 'NC', 'SP', 'VMN', 'SP', 'NC', 'AQ', 'AQ', 'RG', 'SP', 'DI', 'NC', 'SP', 'NC', 'PR', 'VMI', 'DA', 'NC', 'SP', 'DA', 'NC', 'AQ', 'SP', 'DA', 'NC', 'Fp'], ['DA', 'NC', 'SP', 'NC', 'AQ', 'VMI', 'NC', 'RG', 'SP', 'CS', 'DI', 'NC', 'SP', 'NC', 'AQ', 'SP', 'NC', 'SP', 'NC', 'Fpa', 'NP', 'Fpt', 'P0', 'VMS', 'AQ', 'SP', 'VMN', 'DI', 'NC', 'AQ', 'CC', 'VMN', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'CS', 'DA', 'NC', 'PR', 'PP', 'VMI', 'VMI', 'VAN', 'VMP', 'NC', 'SP', 'DA', 'VMP', 'SP', 'NC', 'SP', 'DA', 'NC', 'AQ', 'Fp']]\n",
      "('B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O')\n"
     ]
    }
   ],
   "source": [
    "tokens_list = []\n",
    "pos_list = []\n",
    "labels_list = []\n",
    "\n",
    "for sents in train_sents:\n",
    "    tokens = tuple(i[0] for i in sents)\n",
    "    pos_tags = [i[1] for i in sents]\n",
    "    labels = tuple(i[2] for i in sents)\n",
    "    tokens_list.append(tokens)\n",
    "    pos_list.append(pos_tags)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "print(tokens_list[0])\n",
    "print(pos_list[:4])\n",
    "print(labels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-87810bc2b036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"say\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meow\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"dog\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"say\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"woof\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Achyuta/venv3.6/lib/python3.6/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Achyuta/venv3.6/lib/python3.6/site-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0mstrip_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_numeric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mstrip_non_alphanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Achyuta/venv3.6/lib/python3.6/site-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Achyuta/venv3.6/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Achyuta/venv3.6/lib/python3.6/site-packages/smart_open/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msmart_open_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Achyuta/venv3.6/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mbz2file\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBZ2File\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mbz2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBZ2File\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/bz2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdummy_threading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_bz2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBZ2Compressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBZ2Decompressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_bz2'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "print(model['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CS', 'VMN', 'DD', 'RG', 'Fd', 'VSM', 'Fh', 'PR', 'I', 'RN', 'DI', 'SP', 'DN', 'VSI', 'Fx', 'DT', 'PI', 'VSG', 'PN', 'AO', 'PT', 'Fpa', 'VMS', 'CC', 'VAN', 'Fe', 'VAI', 'Z', 'VMI', 'Fc', 'Fit', 'PD', 'VMM', 'Fpt', 'PP', 'VAS', 'VSS', 'AQ', 'Fz', 'Fs', 'Ft', 'Fat', 'DP', 'VMP', 'NP', 'VSN', 'NC', 'Fp', 'P0', 'VMG', 'Fg', 'Y', 'Faa', 'PX', 'DA', 'VAP', 'Fia', 'VSP']\n",
      "[44, 21, 44, 33, 29, 27, 46, 21, 46, 33, 47]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "NP\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(pos_list)#([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])   \n",
    "print(enc.n_values_)\n",
    "print(enc.feature_indices_)\n",
    "print(enc.transform([('NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp')]).toarray())\n",
    "'''\n",
    "from numpy import argmax\n",
    "import itertools\n",
    "\n",
    "# define input string\n",
    "data = ['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp']\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "unq_pos_list = list(itertools.chain.from_iterable(pos_list))\n",
    "unq_pos_list = list(set(unq_pos_list))\n",
    "print(unq_pos_list)\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unq_pos_list))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(unq_pos_list))\n",
    "\n",
    "def onehot_encode(data,unq_pos_list,char_to_int):\n",
    "    # integer encode input data\n",
    "    integer_encoded = [char_to_int[pos] for pos in data]\n",
    "    print(integer_encoded)\n",
    "\n",
    "    # one hot encode\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "\t    letter = [0 for _ in range(len(unq_pos_list))]\n",
    "\t    letter[value] = 1\n",
    "\t    onehot_encoded.append(letter)\n",
    "    #print(onehot_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "onehot_encoded = onehot_encode(data,unq_pos_list,char_to_int)\n",
    "print(onehot_encoded)\n",
    "\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train & evaluation wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps) \n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch = next(data_iterator)\n",
    "        print(train_batch)\n",
    "        print(labels_batch)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.data.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    \n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch = next(data_iterator)\n",
    "        \n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.data.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    return metrics_mean\n",
    "\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        train_data_iterator = data_loader.data_iterator(train_data, params, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, metrics, params, num_steps)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_data_iterator = data_loader.data_iterator(val_data, params, shuffle=False)\n",
    "        val_metrics = evaluate(model, loss_fn, val_data_iterator, metrics, params, num_steps)\n",
    "        \n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()}, \n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "            \n",
    "        # If best_eval, best_save_path        \n",
    "        if is_best:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "- done.\n",
      "Starting training for 10 epoch(s)\n",
      "100%|██████████| 2/2 [00:00<00:00, 25.21it/s, loss=2.117]\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.03it/s, loss=2.083]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 32.15it/s, loss=2.050]\n",
      "100%|██████████| 2/2 [00:00<00:00, 41.43it/s, loss=2.016]\n",
      "100%|██████████| 2/2 [00:00<00:00, 42.25it/s, loss=1.981]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 45.51it/s, loss=1.943]\n",
      "100%|██████████| 2/2 [00:00<00:00, 44.50it/s, loss=1.901]\n",
      "100%|██████████| 2/2 [00:00<00:00, 44.58it/s, loss=1.855]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s, loss=1.834]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 45.87it/s, loss=1.803]\n",
      "100%|██████████| 2/2 [00:00<00:00, 41.79it/s, loss=1.744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n",
      "tensor([[110, 115, 116, 117, 118,   1,   9, 114, 119,  53, 120, 121, 122, 123,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 22,   1,  23,  24,  11,   9,  25,  26,   9,  27,  28,  29,  30,  31,\n",
      "          32,  33,  34,  35,  36,  37,  38,  39,  35,  13,  35,  40,   9,  41,\n",
      "          21,  35],\n",
      "        [124, 125, 126, 127, 128,   7, 129, 130,   7, 131, 132, 118,   1,   9,\n",
      "         123, 107,  93, 133, 134, 135, 136, 137, 138, 139,  21, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,   6,  85,  86,  87,   1,  88,  89,  90,  11,  91,  92,  93,  94,\n",
      "          95,  93,  96,  93,  13,  97,  21, 366, 366, 366, 366, 366, 366, 366,\n",
      "         366, 366],\n",
      "        [ 61,  98,  99, 100, 101,  78,   7, 102, 103, 104,   1, 105,  11, 106,\n",
      "         107,  63, 108,   7, 109,   7, 110,  68, 111,   1, 112, 113, 114,  21,\n",
      "         366, 366]])\n",
      "tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
      "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
      "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1]])\n",
      "tensor([[ 49,  50,   9,  51,   1,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          21, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366],\n",
      "        [ 61,  77,  78,  79,  80,  67,  68,  81,  11,   9,  12,  25,  13,   9,\n",
      "          82,  83,   1,  84,  16,  17,  11,  19,  20,  21, 366],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   9,  15,   1,  16,  17,  18,  19,  20,  21, 366],\n",
      "        [ 61,   8,  62,  63,   9,  64,   1,   9,  65,  66,   1,  67,  68,  69,\n",
      "          70,  71,  11,   9,  72,  73,  74,  75,   1,  76,  21],\n",
      "        [ 42,   4,  18,   9,  43,   1,  44,   7,  45,  46,  11,  47,  48,  21,\n",
      "         366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          2,  0,  0,  0,  0,  0, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
      "          0,  2,  0,  0,  0,  1,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1]])\n",
      "Checkpoint Directory exists! \n"
     ]
    }
   ],
   "source": [
    "restore_file=None\n",
    "\n",
    "# Load the parameters from json file\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "    \n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "        \n",
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "    \n",
    "# load data\n",
    "data_loader = DataLoader(data_dir, params)\n",
    "data = data_loader.load_data(['train', 'val'], data_dir)\n",
    "train_data = data['train']\n",
    "val_data = data['val']\n",
    "\n",
    "# add word2vec gensim\n",
    "#from gensim.models import KeyedVectors\n",
    "#filename = '{}/GoogleNews-vectors-negative300.bin'.format(word2vec_dir)\n",
    "#model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "#OR\n",
    "'''\n",
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "with open('{}/train/sentences.tsv'.format(data_dir),'r') as f:\n",
    "    g_data = f.read_lines()\n",
    "    g_data = [i.split() for i in g_data]\n",
    "    print(g_data)\n",
    "sentences = g_data\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "'''\n",
    "\n",
    "# specify the train and val dataset sizes\n",
    "params.train_size = train_data['size']\n",
    "params.val_size = val_data['size']\n",
    "\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "# Define the model and optimizer\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    \n",
    "# fetch loss function and metrics\n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir,\n",
    "                    restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  bzip2-doc\n",
      "The following NEW packages will be installed:\n",
      "  bzip2-doc libbz2-dev\n",
      "0 upgraded, 2 newly installed, 0 to remove and 72 not upgraded.\n",
      "Need to get 324 kB of archives.\n",
      "After this operation, 556 kB of additional disk space will be used.\n",
      "Do you want to continue? [Y/n] ^C\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install libbz2-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [[0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   9,\n",
       "   15,\n",
       "   1,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21],\n",
       "  [22,\n",
       "   1,\n",
       "   23,\n",
       "   24,\n",
       "   11,\n",
       "   9,\n",
       "   25,\n",
       "   26,\n",
       "   9,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   35,\n",
       "   13,\n",
       "   35,\n",
       "   40,\n",
       "   9,\n",
       "   41,\n",
       "   21,\n",
       "   35],\n",
       "  [42, 4, 18, 9, 43, 1, 44, 7, 45, 46, 11, 47, 48, 21],\n",
       "  [49, 50, 9, 51, 1, 52, 53, 54, 55, 56, 57, 58, 59, 60, 21],\n",
       "  [61,\n",
       "   8,\n",
       "   62,\n",
       "   63,\n",
       "   9,\n",
       "   64,\n",
       "   1,\n",
       "   9,\n",
       "   65,\n",
       "   66,\n",
       "   1,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   11,\n",
       "   9,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   1,\n",
       "   76,\n",
       "   21],\n",
       "  [61,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   67,\n",
       "   68,\n",
       "   81,\n",
       "   11,\n",
       "   9,\n",
       "   12,\n",
       "   25,\n",
       "   13,\n",
       "   9,\n",
       "   82,\n",
       "   83,\n",
       "   1,\n",
       "   84,\n",
       "   16,\n",
       "   17,\n",
       "   11,\n",
       "   19,\n",
       "   20,\n",
       "   21],\n",
       "  [61,\n",
       "   6,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   1,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   11,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   93,\n",
       "   96,\n",
       "   93,\n",
       "   13,\n",
       "   97,\n",
       "   21],\n",
       "  [61,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   78,\n",
       "   7,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   1,\n",
       "   105,\n",
       "   11,\n",
       "   106,\n",
       "   107,\n",
       "   63,\n",
       "   108,\n",
       "   7,\n",
       "   109,\n",
       "   7,\n",
       "   110,\n",
       "   68,\n",
       "   111,\n",
       "   1,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   21],\n",
       "  [110, 115, 116, 117, 118, 1, 9, 114, 119, 53, 120, 121, 122, 123, 21],\n",
       "  [124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   7,\n",
       "   129,\n",
       "   130,\n",
       "   7,\n",
       "   131,\n",
       "   132,\n",
       "   118,\n",
       "   1,\n",
       "   9,\n",
       "   123,\n",
       "   107,\n",
       "   93,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   21]],\n",
       " 'labels': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 6, 0, 0, 0, 2, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
       "  [0,\n",
       "   5,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   7,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 5, 0, 0, 0, 0, 0]],\n",
       " 'size': 10}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7427669902912621, 'loss': 1.8864147663116455}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "restore_file='best'\n",
    "\n",
    "# Load the parameters\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()     # use GPU is available\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "# load data\n",
    "data_loader = DataLoader(data_dir, params)\n",
    "data = data_loader.load_data(['test'], data_dir)\n",
    "test_data = data['test']\n",
    "\n",
    "# specify the test set size\n",
    "params.test_size = test_data['size']\n",
    "test_data_iterator = data_loader.data_iterator(test_data, params)\n",
    "\n",
    "# Define the model\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "    \n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Reload weights from the saved file\n",
    "utils.load_checkpoint(os.path.join(model_dir, restore_file + '.pth.tar'), model)\n",
    "\n",
    "# Evaluate\n",
    "num_steps = (params.test_size + 1) // params.batch_size\n",
    "test_metrics = evaluate(model, loss_fn, test_data_iterator, metrics, params, num_steps)\n",
    "save_path = os.path.join(model_dir, \"metrics_test_{}.json\".format(restore_file))\n",
    "utils.save_dict_to_json(test_metrics, save_path)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 790kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/achyuta/Achyuta/venv3.6/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Collecting singledispatch (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/achyuta/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
