{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below load dependency pkgs. and then create manually a new directory under experiments like base_model having params.json. this json file contains hyperparameter related to model, user can tweak these parameters for finding best model. and based on new name , change below model_dir path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "# dependancy packages.\n",
    "import utils\n",
    "import model.net_w2v as net\n",
    "from model.data_loader import DataLoader\n",
    "\n",
    "\n",
    "data_dir='data/small'\n",
    "model_dir='experiments/base_model'\n",
    "word2vec_dir='experiments/word2vec' # keep GoogleNews-vectors-negative300.bin inside this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Let's use CoNLL 2002 data to build a NER system\n",
    "# \n",
    "# CoNLL2002 corpus is available in NLTK. We use Spanish data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "nltk.corpus.conll2002.fileids()\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "train_sents = [val for val in train_sents if len(val)!=0]\n",
    "train_sents = train_sents[:5000]\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "test_sents = [val for val in test_sents if len(val)!=0]\n",
    "test_sents = test_sents[:1000]\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('melbourne', '(', 'australia', ')', ',', '25', 'may', '(', 'efe', ')', '.')\n",
      "[['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp'], ['Fg'], ['DA', 'NC', 'AQ', 'SP', 'NC', 'Fc', 'VMI', 'NC', 'Fc', 'VMI', 'RG', 'DA', 'NC', 'SP', 'VMN', 'NC', 'SP', 'VMN', 'SP', 'NC', 'AQ', 'AQ', 'RG', 'SP', 'DI', 'NC', 'SP', 'NC', 'PR', 'VMI', 'DA', 'NC', 'SP', 'DA', 'NC', 'AQ', 'SP', 'DA', 'NC', 'Fp'], ['DA', 'NC', 'SP', 'NC', 'AQ', 'VMI', 'NC', 'RG', 'SP', 'CS', 'DI', 'NC', 'SP', 'NC', 'AQ', 'SP', 'NC', 'SP', 'NC', 'Fpa', 'NP', 'Fpt', 'P0', 'VMS', 'AQ', 'SP', 'VMN', 'DI', 'NC', 'AQ', 'CC', 'VMN', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'CS', 'DA', 'NC', 'PR', 'PP', 'VMI', 'VMI', 'VAN', 'VMP', 'NC', 'SP', 'DA', 'VMP', 'SP', 'NC', 'SP', 'DA', 'NC', 'AQ', 'Fp']]\n",
      "('B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O')\n"
     ]
    }
   ],
   "source": [
    "tokens_word2vec_list = []\n",
    "tr_tokens_list = []\n",
    "tr_pos_list = []\n",
    "tr_labels_list = []\n",
    "ts_tokens_list = []\n",
    "ts_pos_list = []\n",
    "ts_labels_list = []\n",
    "\n",
    "for sents in train_sents:\n",
    "    tokens = tuple(i[0].lower() for i in sents)\n",
    "    pos_tags = [i[1] for i in sents]\n",
    "    labels = tuple(i[2] for i in sents)\n",
    "    tokens_word2vec_list.append(tokens)\n",
    "    tr_tokens_list.append(tokens)\n",
    "    tr_pos_list.append(pos_tags)\n",
    "    tr_labels_list.append(labels)\n",
    "\n",
    "for sents in test_sents:\n",
    "    tokens = tuple(i[0].lower() for i in sents)\n",
    "    pos_tags = [i[1] for i in sents]\n",
    "    labels = tuple(i[2] for i in sents)\n",
    "    tokens_word2vec_list.append(tokens)\n",
    "    ts_tokens_list.append(tokens)\n",
    "    ts_pos_list.append(pos_tags)\n",
    "    ts_labels_list.append(labels)\n",
    "\n",
    "print(tr_tokens_list[0])\n",
    "print(tr_pos_list[:4])\n",
    "print(tr_labels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(637027, 960020)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = tokens_word2vec_list#[[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "emb_model = Word2Vec(min_count=1)\n",
    "emb_model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "emb_model.train(sentences, total_examples=emb_model.corpus_count, epochs=emb_model.iter)  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.08836885e-02,  5.52735366e-02, -2.91795712e-02,  5.13534583e-02,\n",
       "       -8.55285898e-02, -1.45775989e-01,  6.77944720e-02,  1.05020646e-02,\n",
       "        5.35919964e-02,  9.33396816e-02,  5.80378175e-02, -1.29253387e-01,\n",
       "        7.27205426e-02, -1.37003893e-02, -4.38482203e-02, -2.40821242e-02,\n",
       "        4.72355820e-02, -1.49125792e-02,  1.17227465e-01,  1.26715124e-01,\n",
       "       -1.69198975e-01, -2.81737838e-03, -4.11506966e-02,  4.19704132e-02,\n",
       "        3.66104729e-02,  2.21566427e-02,  6.25741407e-02,  4.48836274e-02,\n",
       "        4.61159162e-02, -3.42506170e-02,  6.44492656e-02,  8.01340584e-03,\n",
       "        9.37123597e-03, -5.86786903e-02,  6.48811366e-03, -8.50597993e-02,\n",
       "        1.30279418e-02, -1.39118321e-02, -2.99086794e-02,  3.99441458e-02,\n",
       "        6.05207831e-02,  4.67889756e-03, -9.84974951e-03, -2.37853061e-02,\n",
       "       -7.96138644e-02,  7.95610026e-02, -5.96938580e-02,  5.31852320e-02,\n",
       "       -2.99377143e-02, -3.87722357e-05,  5.02623990e-03,  6.83693960e-02,\n",
       "        5.47746122e-02, -1.91719849e-02, -4.08048071e-02, -4.96592224e-02,\n",
       "       -1.90153867e-02,  1.01025747e-02, -4.42400984e-02,  5.14045469e-02,\n",
       "       -3.14656161e-02,  4.77441438e-02, -2.81048901e-02,  9.81199667e-02,\n",
       "        7.32746199e-02, -2.22112816e-02,  1.03848889e-01, -4.49284576e-02,\n",
       "       -8.99697095e-02, -1.63034759e-02,  4.53483649e-02, -3.34130749e-02,\n",
       "        2.39021964e-02, -4.69286963e-02,  8.75523537e-02, -7.63589796e-03,\n",
       "        1.16221972e-01,  1.23200789e-01, -3.36403772e-02, -3.44569832e-02,\n",
       "       -2.79812235e-03, -9.75692570e-02, -3.76846753e-02,  2.08720323e-02,\n",
       "       -9.75142121e-02,  1.38598010e-01,  2.99289804e-02,  1.33736640e-01,\n",
       "        2.70064864e-02, -3.48796882e-02, -5.40220216e-02,  1.56428948e-01,\n",
       "        7.79710636e-02, -4.37337942e-02,  4.99096848e-02, -4.80230600e-02,\n",
       "       -7.34853148e-02,  1.27325449e-02, -4.01789024e-02, -2.60156877e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model['australia'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "- done.\n",
      "Building tag vocabulary...\n",
      "- done.\n",
      "Saving vocabularies to file...\n",
      "- done.\n",
      "Characteristics of the dataset:\n",
      "- train_size: 5000\n",
      "- dev_size: 1000\n",
      "- test_size: 1000\n",
      "- vocab_size: 19952\n",
      "- number_of_tags: 9\n",
      "- pad_word: <pad>\n",
      "- pad_tag: O\n",
      "- unk_word: UNK\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build vocabularies of words and tags from datasets\"\"\"\n",
    "\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "parser = {'min_count_word':1,'min_count_tag':1,'data_dir':'data/small'}#argparse.ArgumentParser()\n",
    "#parser.add_argument('--min_count_word', default=1, help=\"Minimum count for words in the dataset\", type=int)\n",
    "#parser.add_argument('--min_count_tag', default=1, help=\"Minimum count for tags in the dataset\", type=int)\n",
    "#parser.add_argument('--data_dir', default='data/small', help=\"Directory containing the dataset\")\n",
    "\n",
    "# Hyper parameters for the vocab\n",
    "PAD_WORD = '<pad>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = 'UNK'\n",
    "\n",
    "\n",
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
    "\n",
    "    Args:\n",
    "        vocab: (iterable object) yields token\n",
    "        txt_path: (stirng) path to vocab file\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict to json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: v for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "\n",
    "def update_vocab(tokens_list, vocab):\n",
    "    \"\"\"Update word and tag vocabulary from dataset\n",
    "\n",
    "    Args:\n",
    "        txt_path: (string) path to file, one sentence per line\n",
    "        vocab: (dict or Counter) with update method\n",
    "\n",
    "    Returns:\n",
    "        dataset_size: (int) number of elements in the dataset\n",
    "    \"\"\"\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    # Build word vocab with train and test datasets\n",
    "    print(\"Building word vocabulary...\")\n",
    "    words = Counter()\n",
    "    size_train_sentences = update_vocab(tr_tokens_list, words)\n",
    "    size_dev_sentences = update_vocab(ts_tokens_list, words)\n",
    "    size_test_sentences = update_vocab(ts_tokens_list, words)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Build tag vocab with train and test datasets\n",
    "    print(\"Building tag vocabulary...\")\n",
    "    tags = Counter()\n",
    "    size_train_tags = update_vocab(tr_labels_list, tags)\n",
    "    size_dev_tags = update_vocab(ts_labels_list, tags)\n",
    "    size_test_tags = update_vocab(ts_labels_list, tags)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Assert same number of examples in datasets\n",
    "    assert size_train_sentences == size_train_tags\n",
    "    assert size_dev_sentences == size_dev_tags\n",
    "    assert size_test_sentences == size_test_tags\n",
    "\n",
    "    # Only keep most frequent tokens\n",
    "    words = [tok for tok, count in words.items() if count >= parser['min_count_word']]\n",
    "    tags = [tok for tok, count in tags.items() if count >= parser['min_count_tag']]\n",
    "\n",
    "    # Add pad tokens\n",
    "    if PAD_WORD not in words: words.append(PAD_WORD)\n",
    "    if PAD_TAG not in tags: tags.append(PAD_TAG)\n",
    "    \n",
    "    # add word for unknown words \n",
    "    words.append(UNK_WORD)\n",
    "\n",
    "    # Save vocabularies to file\n",
    "    print(\"Saving vocabularies to file...\")\n",
    "    save_vocab_to_txt_file(words, os.path.join(parser['data_dir'], 'words.txt'))\n",
    "    save_vocab_to_txt_file(tags, os.path.join(parser['data_dir'], 'tags.txt'))\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Save datasets properties in json file\n",
    "    sizes = {\n",
    "        'train_size': size_train_sentences,\n",
    "        'dev_size': size_dev_sentences,\n",
    "        'test_size': size_test_sentences,\n",
    "        'vocab_size': len(words),\n",
    "        'number_of_tags': len(tags),\n",
    "        'pad_word': PAD_WORD,\n",
    "        'pad_tag': PAD_TAG,\n",
    "        'unk_word': UNK_WORD\n",
    "    }\n",
    "    \n",
    "    save_dict_to_json(sizes, os.path.join(parser['data_dir'], 'dataset_params.json'))\n",
    "\n",
    "    # Logging sizes\n",
    "    to_print = \"\\n\".join(\"- {}: {}\".format(k, v) for k, v in sizes.items())\n",
    "    print(\"Characteristics of the dataset:\\n{}\".format(to_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fs', 'DA', 'PP', 'VSP', 'Y', 'Fp', 'Ft', 'DN', 'Faa', 'Fx', 'RN', 'AO', 'VSN', 'VMM', 'Fat', 'VAN', 'VMG', 'NP', 'Fh', 'Fc', 'VSI', 'VSM', 'NC', 'I', 'Fpt', 'VMS', 'PN', 'RG', 'VAS', 'Fpa', 'VMI', 'Fd', 'Fit', 'VAI', 'Fz', 'VMP', 'Fia', 'DD', 'VAP', 'PR', 'DP', 'DI', 'AQ', 'SP', 'Fg', 'PD', 'PI', 'VSS', 'PT', 'Fe', 'DT', 'CC', 'Z', 'P0', 'VSG', 'CS', 'VMN', 'PX']\n",
      "[17, 29, 17, 24, 19, 52, 22, 29, 22, 24, 5]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "NP\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(pos_list)#([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])   \n",
    "print(enc.n_values_)\n",
    "print(enc.feature_indices_)\n",
    "print(enc.transform([('NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp')]).toarray())\n",
    "'''\n",
    "from numpy import argmax\n",
    "import itertools\n",
    "\n",
    "# define input string\n",
    "data = ['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp']\n",
    "\n",
    "# define universe of possible input values\n",
    "unq_pos_list = list(itertools.chain.from_iterable(tr_pos_list+ts_pos_list))\n",
    "unq_pos_list = list(set(unq_pos_list))\n",
    "print(unq_pos_list)\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unq_pos_list))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(unq_pos_list))\n",
    "\n",
    "def onehot_encode(data,unq_pos_list,char_to_int):\n",
    "    # integer encode input data\n",
    "    integer_encoded = [char_to_int[pos] for pos in data]\n",
    "    print(integer_encoded)\n",
    "\n",
    "    # one hot encode\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "\t    letter = [0 for _ in range(len(unq_pos_list))]\n",
    "\t    letter[value] = 1\n",
    "\t    onehot_encoded.append(letter)\n",
    "    #print(onehot_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "onehot_encoded = onehot_encode(data,unq_pos_list,char_to_int)\n",
    "print(onehot_encoded)\n",
    "\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train & evaluation wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader(object):\n",
    "    def data_iterator(self, data_dir, data, params, shuffle=False):\n",
    "        \"\"\"\n",
    "        Returns a generator that yields batches data with labels. Batch size is params.batch_size. Expires after one\n",
    "        pass over the data.\n",
    "\n",
    "        Args:\n",
    "            data: (dict) contains data which has keys 'data', 'labels' and 'size'\n",
    "            params: (Params) hyperparameters of the training process.\n",
    "            shuffle: (bool) whether the data should be shuffled\n",
    "\n",
    "        Yields:\n",
    "            batch_data: (Variable) dimension batch_size x seq_len with the sentence data\n",
    "            batch_labels: (Variable) dimension batch_size x seq_len with the corresponding labels\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(230)\n",
    "            random.shuffle(order)\n",
    "\n",
    "        # loading dataset_params\n",
    "        json_path = os.path.join(data_dir, 'dataset_params.json')\n",
    "        assert os.path.isfile(json_path), \"No json file found at {}, run build_vocab.py\".format(json_path)\n",
    "        dataset_params = utils.Params(json_path)        \n",
    "\n",
    "        # loading vocab (we require this to map words to their indices)\n",
    "        vocab_path = os.path.join(data_dir, 'words.txt')\n",
    "        vocab = {}\n",
    "        with open(vocab_path) as f:\n",
    "            for i, l in enumerate(f.read().splitlines()):\n",
    "                vocab[l] = i\n",
    "\n",
    "        # setting the indices for UNKnown words and PADding symbols\n",
    "        self.unk_ind = vocab[dataset_params.unk_word]\n",
    "        self.pad_ind = vocab[dataset_params.pad_word]\n",
    "        \n",
    "        tr_tokens_list_ind = []\n",
    "        for sent_token_list in data['data']:\n",
    "            s = [vocab[token] if token in vocab else self.unk_ind for token in sent_token_list]\n",
    "            tr_tokens_list_ind.append(s)\n",
    "        data['data'] = tr_tokens_list_ind\n",
    "        \n",
    "\n",
    "        # one pass over data\n",
    "        for i in range((data['size']+1)//params.batch_size):\n",
    "            # fetch sentences and tags\n",
    "            batch_sentences = [data['data'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "            batch_tags = [data['labels'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "\n",
    "            # compute length of longest sentence in batch\n",
    "            batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "            # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "            # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "            batch_data = self.pad_ind*np.ones((len(batch_sentences), batch_max_len))\n",
    "            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "            \n",
    "            #print(\"------------------------------\")\n",
    "            #print(np.array(batch_sentences).shape)\n",
    "            #print(batch_data.shape)\n",
    "            #print(batch_labels.shape)\n",
    "            #print(np.array(batch_tags).shape)\n",
    "            #print(batch_sentences)\n",
    "            #print(\"===================\")\n",
    "            #print(batch_tags)\n",
    "\n",
    "            # copy the data to the numpy array\n",
    "            for j in range(len(batch_sentences)):\n",
    "                cur_len = len(batch_sentences[j])\n",
    "                #print(cur_len)\n",
    "                #print(batch_data[j][:cur_len].shape)\n",
    "                batch_data[j][:cur_len] = batch_sentences[j]\n",
    "                batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "            # since all data are indices, we convert them to torch LongTensors\n",
    "            batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "            #print(type(batch_data))\n",
    "            #print(type(batch_labels))\n",
    "            #batch_data = batch_data.to(dtype=torch.long)\n",
    "            #batch_labels = batch_labels.to(dtype=torch.long)\n",
    "\n",
    "            # shift tensors to GPU if available\n",
    "            if params.cuda:\n",
    "                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "            # convert them to Variables to record operations in the computational graph\n",
    "            batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "            #print(type(batch_data))\n",
    "            #print(type(batch_labels))\n",
    "            #print(batch_data.dtype)\n",
    "            #print(batch_data[0].dtype)\n",
    "            #print(batch_data[0])\n",
    "\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps) \n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch = next(data_iterator)\n",
    "        #print(\"-------------------------\")\n",
    "        #print(train_batch.shape)\n",
    "        #print(labels_batch.shape)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.data.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    \n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch = next(data_iterator)\n",
    "        \n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.data.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    return metrics_mean\n",
    "\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        dl = data_loader()\n",
    "        train_data_iterator = dl.data_iterator(data_dir, train_data, params, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, metrics, params, num_steps)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_data_iterator = dl.data_iterator(data_dir, val_data, params, shuffle=False)\n",
    "        val_metrics = evaluate(model, loss_fn, val_data_iterator, metrics, params, num_steps)\n",
    "        \n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()}, \n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "            \n",
    "        # If best_eval, best_save_path        \n",
    "        if is_best:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "- done.\n",
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:109: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 29, 17, 24, 19, 52, 22, 29, 22, 24, 5]\n",
      "[44]\n",
      "[1, 22, 42, 43, 22, 19, 30, 22, 19, 30, 27, 1, 22, 43, 56, 22, 43, 56, 43, 22, 42, 42, 27, 43, 41, 22, 43, 22, 39, 30, 1, 22, 43, 1, 22, 42, 43, 1, 22, 5]\n",
      "[1, 22, 43, 22, 42, 30, 22, 27, 43, 55, 41, 22, 43, 22, 42, 43, 22, 43, 22, 29, 17, 24, 53, 25, 42, 43, 56, 41, 22, 42, 51, 56, 1, 22, 43, 1, 22, 43, 1, 22, 43, 55, 1, 22, 39, 2, 30, 30, 15, 35, 22, 43, 1, 35, 43, 22, 43, 1, 22, 42, 5]\n",
      "[37, 22, 42, 30, 41, 22, 43, 22, 19, 22, 43, 1, 39, 33, 3, 35, 43, 27, 43, 41, 22, 43, 22, 19, 51, 30, 22, 43, 26, 43, 22, 51, 13, 27, 35, 51, 27, 13, 19, 22, 39, 30, 43, 22, 43, 22, 51, 22, 42, 5]\n",
      "[1, 22, 19, 52, 22, 29, 17, 24, 5]\n",
      "[44]\n",
      "[1, 22, 49, 22, 42, 49, 43, 22, 42, 43, 22, 43, 22, 30, 43, 56, 41, 52, 43, 26, 43, 1, 11, 22, 43, 37, 22, 43, 22, 43, 41, 22, 43, 52, 19, 30, 27, 42, 22, 42, 19, 42, 43, 22, 43, 22, 42, 5]\n",
      "[22, 42, 30, 43, 1, 22, 1, 22, 42, 19, 39, 30, 1, 42, 22, 43, 1, 22, 43, 22, 42, 43, 56, 22, 43, 22, 51, 22, 19, 27, 55, 22, 43, 22, 19, 51, 39, 27, 30, 56, 27, 1, 42, 22, 43, 22, 43, 22, 5]\n",
      "[1, 22, 30, 56, 1, 22, 49, 22, 49, 19, 39, 1, 42, 22, 30, 27, 27, 43, 7, 22, 43, 22, 43, 22, 19, 43, 56, 22, 43, 22, 43, 22, 19, 22, 42, 51, 22, 42, 19, 55, 1, 22, 42, 30, 27, 43, 22, 43, 41, 22, 43, 22, 19, 30, 1, 22, 43, 22, 42, 5]\n",
      "exception word not have embeddings :  <pad>\n",
      "exception word not have embeddings :  UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epoch(s)\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.35it/s, loss=2.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.49it/s, loss=2.276]\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.01it/s, loss=2.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.66it/s, loss=2.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.34it/s, loss=2.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.15it/s, loss=2.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.95it/s, loss=2.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.48it/s, loss=2.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.35it/s, loss=2.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.54it/s, loss=2.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    }
   ],
   "source": [
    "restore_file=None\n",
    "\n",
    "# Load the parameters from json file\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "    \n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "        \n",
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "    \n",
    "# load data\n",
    "#data_loader = DataLoader(data_dir, params)\n",
    "#data = data_loader.load_data(['train', 'val'], data_dir)\n",
    "#train_data = data['train']\n",
    "#val_data = data['val']\n",
    "params.vocab_size=sizes['vocab_size']\n",
    "params.number_of_tags = sizes['number_of_tags']\n",
    "\n",
    "# add word2vec\n",
    "'''\n",
    "train_data = {'data':[[],[]],'labels':[[]],'size':10}\n",
    "'''\n",
    "tr_data = None\n",
    "tr_pos = None\n",
    "ts_data = None\n",
    "ts_pos = None\n",
    "def preprocess_data(tokens_list,pos_list):\n",
    "    tr_data = []\n",
    "    for v in tokens_list[:5]:\n",
    "        tmp_tr_data = []\n",
    "        for v_in in v:\n",
    "            tmp_tr_data.append(emb_model[v_in.lower()].tolist()) #.astype(np.float64)\n",
    "        tr_data.append(tmp_tr_data)\n",
    "    \n",
    "    tr_pos = [onehot_encode(v,unq_pos_list,char_to_int) for v in pos_list[:5]]\n",
    "    return tr_data,tr_pos\n",
    "\n",
    "tr_data,tr_pos = preprocess_data(tr_tokens_list,tr_pos_list)\n",
    "ts_data,ts_pos = preprocess_data(ts_tokens_list,ts_pos_list)\n",
    "\n",
    "\n",
    "# loading tags (we require this to map tags to their indices)\n",
    "tags_path = os.path.join(data_dir, 'tags.txt')\n",
    "tag_map = {}\n",
    "with open(tags_path) as f:\n",
    "    for i, t in enumerate(f.read().splitlines()):\n",
    "        tag_map[t] = i\n",
    "\n",
    "def preprocess_label(tag_map, labels_list):\n",
    "    fnl_labels = []\n",
    "    for label_sent in labels_list:\n",
    "        tmp_label = []\n",
    "        for label in label_sent:\n",
    "            tmp_label.append(tag_map[label])\n",
    "        fnl_labels.append(tmp_label)\n",
    "    return fnl_labels\n",
    "\n",
    "tr_label_list = preprocess_label(tag_map, tr_labels_list)\n",
    "ts_label_list = preprocess_label(tag_map, ts_labels_list)\n",
    "\n",
    "tr_tokens_list_1 = []\n",
    "tr_label_list_1 = []\n",
    "for i,j in zip(tr_tokens_list,tr_label_list):\n",
    "    if len(i) != len(j):\n",
    "        print(\"not equal length : \")\n",
    "        continue\n",
    "    tr_tokens_list_1.append(i)\n",
    "    tr_label_list_1.append(j)\n",
    "\n",
    "ts_tokens_list_1 = []\n",
    "ts_label_list_1 = []\n",
    "for i,j in zip(ts_tokens_list,ts_label_list):\n",
    "    if len(i) != len(j):\n",
    "        print(\"not equal length : \")\n",
    "        continue\n",
    "    ts_tokens_list_1.append(i)\n",
    "    ts_label_list_1.append(j)\n",
    "\n",
    "train_data = {'data':tr_tokens_list_1,'labels':tr_label_list_1[:5],'size':5}\n",
    "val_data = {'data':ts_tokens_list_1,'labels':ts_label_list_1[:5],'size':5}\n",
    "\n",
    "# specify the train and val dataset sizes\n",
    "params.train_size = train_data['size']\n",
    "params.val_size = val_data['size']\n",
    "\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "# loading vocab (we require this to map words to their indices)\n",
    "vocab_path = os.path.join(data_dir, 'words.txt')\n",
    "vocab = {}\n",
    "with open(vocab_path) as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        vocab[l] = i\n",
    "\n",
    "weight = np.zeros((len(vocab),params.embedding_dim))\n",
    "vocab_inv = {y:x for x,y in vocab.items()}\n",
    "for ind,val in vocab_inv.items():\n",
    "    try:\n",
    "        weight[ind][:]= emb_model[val]\n",
    "    except:\n",
    "        print(\"exception word not have embeddings : \",val)\n",
    "\n",
    "# Define the model and optimizer\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "#weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "model.init_word_embeddings(torch.FloatTensor(weight).cuda())\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    \n",
    "# fetch loss function and metrics\n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir,\n",
    "                    restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir,restore_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8383233532934131, 'loss': 2.124668836593628}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "restore_file='best'\n",
    "\n",
    "# Load the parameters\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()     # use GPU is available\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "params.vocab_size=sizes['vocab_size']\n",
    "params.number_of_tags = sizes['number_of_tags']\n",
    "\n",
    "# load data\n",
    "#data_loader = DataLoader(data_dir, params)\n",
    "#data = data_loader.load_data(['test'], data_dir)\n",
    "#test_data = data['test']\n",
    "\n",
    "test_data = val_data\n",
    "\n",
    "# specify the test set size\n",
    "params.test_size = test_data['size']\n",
    "#test_data_iterator = data_loader.data_iterator(test_data, params)\n",
    "dll = data_loader()\n",
    "test_data_iterator = dll.data_iterator(data_dir, test_data, params)\n",
    "\n",
    "# Define the model\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "    \n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Reload weights from the saved file\n",
    "utils.load_checkpoint(os.path.join(model_dir, restore_file + '.pth.tar'), model)\n",
    "\n",
    "# Evaluate\n",
    "num_steps = (params.test_size + 1) // params.batch_size\n",
    "test_metrics = evaluate(model, loss_fn, test_data_iterator, metrics, params, num_steps)\n",
    "save_path = os.path.join(model_dir, \"metrics_test_{}.json\".format(restore_file))\n",
    "utils.save_dict_to_json(test_metrics, save_path)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
