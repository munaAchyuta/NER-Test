{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below load dependency pkgs. and then create manually a new directory under experiments like base_model having params.json. this json file contains hyperparameter related to model, user can tweak these parameters for finding best model. and based on new name , change below model_dir path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import random\n",
    "from seqeval.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# dependancy packages.\n",
    "import utils\n",
    "import model.net_w2v_pos as net\n",
    "from model.data_loader import DataLoader\n",
    "from model.net_w2v_pos import flat_accc\n",
    "\n",
    "\n",
    "data_dir='data/small'\n",
    "model_dir='experiments/base_model2'\n",
    "word2vec_dir='experiments/word2vec' # keep GoogleNews-vectors-negative300.bin inside this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Let's use CoNLL 2002 data to build a NER system\n",
    "# \n",
    "# CoNLL2002 corpus is available in NLTK. We use Spanish data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "nltk.corpus.conll2002.fileids()\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "train_sents = [val for val in train_sents if len(val)!=0]\n",
    "train_sents = train_sents[:5000]\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "test_sents = [val for val in test_sents if len(val)!=0]\n",
    "test_sents = test_sents[:1000]\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert sentences to list of tokens, pos , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_word2vec_list = []\n",
    "tr_tokens_list = []\n",
    "tr_pos_list = []\n",
    "tr_labels_list = []\n",
    "ts_tokens_list = []\n",
    "ts_pos_list = []\n",
    "ts_labels_list = []\n",
    "\n",
    "def preprocess_data(x,tokens_word2vec_list):\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    for sents in x:\n",
    "        tokens = tuple(i[0].lower() for i in sents)\n",
    "        pos_tags = [i[1] for i in sents]\n",
    "        labels = tuple(i[2] for i in sents)\n",
    "        tokens_word2vec_list.append(tokens)\n",
    "        a.append(tokens)\n",
    "        b.append(pos_tags)\n",
    "        c.append(labels)\n",
    "    return tokens_word2vec_list,a,b,c\n",
    "\n",
    "tokens_word2vec_list,tr_tokens_list,tr_pos_list,tr_labels_list = preprocess_data(train_sents,tokens_word2vec_list)\n",
    "tokens_word2vec_list,ts_tokens_list,ts_pos_list,ts_labels_list = preprocess_data(test_sents,tokens_word2vec_list)\n",
    "#print(tr_tokens_list[0])\n",
    "#print(tr_pos_list[:4])\n",
    "#print(tr_labels_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "collected 19950 word types from a corpus of 192004 raw words and 6000 sentences\n",
      "Loading a fresh vocabulary\n",
      "effective_min_count=1 retains 19950 unique words (100% of original 19950, drops 0)\n",
      "effective_min_count=1 leaves 192004 word corpus (100% of original 192004, drops 0)\n",
      "deleting the raw counts dictionary of 19950 items\n",
      "sample=0.001 downsamples 32 most-common words\n",
      "downsampling leaves estimated 127370 word corpus (66.3% of prior 192004)\n",
      "estimated required memory for 19950 words and 100 dimensions: 25935000 bytes\n",
      "resetting layer weights\n",
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n",
      "training model with 3 workers on 19950 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 192004 raw words (127330 effective words) took 0.1s, 1214780 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 192004 raw words (127521 effective words) took 0.1s, 1355206 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 192004 raw words (127484 effective words) took 0.1s, 1339507 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 192004 raw words (127445 effective words) took 0.1s, 1373093 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 192004 raw words (127247 effective words) took 0.1s, 1297432 effective words/s\n",
      "training on a 960020 raw words (637027 effective words) took 0.5s, 1251150 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(637027, 960020)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = tokens_word2vec_list#[[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "emb_model = Word2Vec(min_count=1)\n",
    "emb_model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "emb_model.train(sentences, total_examples=emb_model.corpus_count, epochs=emb_model.iter)  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_model['australia'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabularies of words and tags from datasets.\n",
    "# why ? bcs here neural networks input demands vocabulary size and vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "- done.\n",
      "Building tag vocabulary...\n",
      "- done.\n",
      "Saving vocabularies to file...\n",
      "- done.\n",
      "Characteristics of the dataset:\n",
      "- train_size: 5000\n",
      "- dev_size: 1000\n",
      "- test_size: 1000\n",
      "- vocab_size: 19952\n",
      "- number_of_tags: 9\n",
      "- pad_word: <pad>\n",
      "- pad_tag: O\n",
      "- unk_word: UNK\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build vocabularies of words and tags from datasets\"\"\"\n",
    "\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "parser = {'min_count_word':1,'min_count_tag':1,'data_dir':'data/small'}#argparse.ArgumentParser()\n",
    "#parser.add_argument('--min_count_word', default=1, help=\"Minimum count for words in the dataset\", type=int)\n",
    "#parser.add_argument('--min_count_tag', default=1, help=\"Minimum count for tags in the dataset\", type=int)\n",
    "#parser.add_argument('--data_dir', default='data/small', help=\"Directory containing the dataset\")\n",
    "\n",
    "# Hyper parameters for the vocab\n",
    "PAD_WORD = '<pad>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = 'UNK'\n",
    "\n",
    "\n",
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
    "\n",
    "    Args:\n",
    "        vocab: (iterable object) yields token\n",
    "        txt_path: (stirng) path to vocab file\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict to json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: v for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "\n",
    "def update_vocab(tokens_list, vocab):\n",
    "    \"\"\"Update word and tag vocabulary from dataset\n",
    "\n",
    "    Args:\n",
    "        txt_path: (string) path to file, one sentence per line\n",
    "        vocab: (dict or Counter) with update method\n",
    "\n",
    "    Returns:\n",
    "        dataset_size: (int) number of elements in the dataset\n",
    "    \"\"\"\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    # Build word vocab with train and test datasets\n",
    "    print(\"Building word vocabulary...\")\n",
    "    words = Counter()\n",
    "    size_train_sentences = update_vocab(tr_tokens_list, words)\n",
    "    size_dev_sentences = update_vocab(ts_tokens_list, words)\n",
    "    size_test_sentences = update_vocab(ts_tokens_list, words)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Build tag vocab with train and test datasets\n",
    "    print(\"Building tag vocabulary...\")\n",
    "    tags = Counter()\n",
    "    size_train_tags = update_vocab(tr_labels_list, tags)\n",
    "    size_dev_tags = update_vocab(ts_labels_list, tags)\n",
    "    size_test_tags = update_vocab(ts_labels_list, tags)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Assert same number of examples in datasets\n",
    "    assert size_train_sentences == size_train_tags\n",
    "    assert size_dev_sentences == size_dev_tags\n",
    "    assert size_test_sentences == size_test_tags\n",
    "\n",
    "    # Only keep most frequent tokens\n",
    "    words = [tok for tok, count in words.items() if count >= parser['min_count_word']]\n",
    "    tags = [tok for tok, count in tags.items() if count >= parser['min_count_tag']]\n",
    "\n",
    "    # Add pad tokens\n",
    "    if PAD_WORD not in words: words.append(PAD_WORD)\n",
    "    if PAD_TAG not in tags: tags.append(PAD_TAG)\n",
    "    \n",
    "    # add word for unknown words \n",
    "    words.append(UNK_WORD)\n",
    "\n",
    "    # Save vocabularies to file\n",
    "    print(\"Saving vocabularies to file...\")\n",
    "    save_vocab_to_txt_file(words, os.path.join(parser['data_dir'], 'words.txt'))\n",
    "    save_vocab_to_txt_file(tags, os.path.join(parser['data_dir'], 'tags.txt'))\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Save datasets properties in json file\n",
    "    sizes = {\n",
    "        'train_size': size_train_sentences,\n",
    "        'dev_size': size_dev_sentences,\n",
    "        'test_size': size_test_sentences,\n",
    "        'vocab_size': len(words),\n",
    "        'number_of_tags': len(tags),\n",
    "        'pad_word': PAD_WORD,\n",
    "        'pad_tag': PAD_TAG,\n",
    "        'unk_word': UNK_WORD\n",
    "    }\n",
    "    \n",
    "    save_dict_to_json(sizes, os.path.join(parser['data_dir'], 'dataset_params.json'))\n",
    "\n",
    "    # Logging sizes\n",
    "    to_print = \"\\n\".join(\"- {}: {}\".format(k, v) for k, v in sizes.items())\n",
    "    print(\"Characteristics of the dataset:\\n{}\".format(to_print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert pos-tags to onehot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "import itertools\n",
    "\n",
    "# define input string\n",
    "data = ['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp']\n",
    "\n",
    "# define universe of possible input values\n",
    "unq_pos_list = list(itertools.chain.from_iterable(tr_pos_list+ts_pos_list))\n",
    "unq_pos_list = list(set(unq_pos_list))\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unq_pos_list))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(unq_pos_list))\n",
    "\n",
    "def onehot_encode(data,unq_pos_list,char_to_int):\n",
    "    # integer encode input data\n",
    "    integer_encoded = [char_to_int[pos] for pos in data]\n",
    "\n",
    "    # one hot encode\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "        letter = [0 for _ in range(len(unq_pos_list))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    return onehot_encoded\n",
    "\n",
    "onehot_encoded = onehot_encode(data,unq_pos_list,char_to_int)\n",
    "\n",
    "# invert encoding\n",
    "#inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train & evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader(object):\n",
    "    def data_iterator(self, data_dir, data, params, shuffle=False):\n",
    "        \"\"\"\n",
    "        Returns a generator that yields batches data with labels. Batch size is params.batch_size. Expires after one\n",
    "        pass over the data.\n",
    "\n",
    "        Args:\n",
    "            data: (dict) contains data which has keys 'data', 'labels' and 'size'\n",
    "            params: (Params) hyperparameters of the training process.\n",
    "            shuffle: (bool) whether the data should be shuffled\n",
    "\n",
    "        Yields:\n",
    "            batch_data: (Variable) dimension batch_size x seq_len with the sentence data\n",
    "            batch_labels: (Variable) dimension batch_size x seq_len with the corresponding labels\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(230)\n",
    "            random.shuffle(order)\n",
    "\n",
    "        # loading dataset_params\n",
    "        json_path = os.path.join(data_dir, 'dataset_params.json')\n",
    "        assert os.path.isfile(json_path), \"No json file found at {}, run build_vocab.py\".format(json_path)\n",
    "        dataset_params = utils.Params(json_path)        \n",
    "\n",
    "        # loading vocab (we require this to map words to their indices)\n",
    "        vocab_path = os.path.join(data_dir, 'words.txt')\n",
    "        vocab = {}\n",
    "        with open(vocab_path) as f:\n",
    "            for i, l in enumerate(f.read().splitlines()):\n",
    "                vocab[l] = i\n",
    "\n",
    "        # setting the indices for UNKnown words and PADding symbols\n",
    "        self.unk_ind = vocab[dataset_params.unk_word]\n",
    "        self.pad_ind = vocab[dataset_params.pad_word]\n",
    "        \n",
    "        tr_tokens_list_ind = []\n",
    "        for sent_token_list in data['data']:\n",
    "            s = [vocab[token] if token in vocab else self.unk_ind for token in sent_token_list]\n",
    "            tr_tokens_list_ind.append(s)\n",
    "        data['data'] = tr_tokens_list_ind\n",
    "        \n",
    "\n",
    "        # one pass over data\n",
    "        for i in range((data['size']+1)//params.batch_size):\n",
    "            # fetch sentences and tags\n",
    "            batch_sentences = [data['data'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "            batch_tags = [data['labels'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "            batch_pos = [data['pos'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "            \n",
    "            # compute length of longest sentence in batch\n",
    "            batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "            # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
    "            # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
    "            batch_data = self.pad_ind*np.ones((len(batch_sentences), batch_max_len))\n",
    "            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "            batch_pos_w = np.zeros((len(batch_sentences), batch_max_len, params.pos_dim))\n",
    "            \n",
    "            # copy the data to the numpy array\n",
    "            for j in range(len(batch_sentences)):\n",
    "                cur_len = len(batch_sentences[j])\n",
    "                batch_data[j][:cur_len] = batch_sentences[j]\n",
    "                batch_labels[j][:cur_len] = batch_tags[j]\n",
    "                batch_pos_w[j][:cur_len] = batch_pos[j]\n",
    "\n",
    "            # since all data are indices, we convert them to torch LongTensors\n",
    "            batch_data, batch_labels, batch_pos_w = torch.LongTensor(batch_data), \\\n",
    "            torch.LongTensor(batch_labels), torch.FloatTensor(batch_pos_w)\n",
    "            \n",
    "            # shift tensors to GPU if available\n",
    "            if params.cuda:\n",
    "                batch_data, batch_labels, batch_pos_w = batch_data.cuda(), batch_labels.cuda(), batch_pos_w.cuda()\n",
    "\n",
    "            # convert them to Variables to record operations in the computational graph\n",
    "            batch_data, batch_labels, batch_pos_w = Variable(batch_data), Variable(batch_labels), Variable(batch_pos_w)\n",
    "            \n",
    "            yield batch_data, batch_labels, batch_pos_w, batch_tags\n",
    "\n",
    "\n",
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps,tag_map_rev):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_sum = 0.0\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    batch_len = 0\n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch, batch_pos_w, batch_tags = next(data_iterator)\n",
    "        \n",
    "        # compute model output and loss\n",
    "        loss = model(train_batch,batch_pos_w,labels_batch)\n",
    "        output_batch = model.forward_decode(train_batch,batch_pos_w,labels_batch)\n",
    "        #print(loss)\n",
    "        #print(output_batch)\n",
    "        #print(\"labels_batch : \",labels_batch)\n",
    "        batch_acc = flat_accc(output_batch, labels_batch)\n",
    "        #print(\"batch_acc : \",batch_acc)\n",
    "        #print(\"f1_score : \",f1_score(batch_tags,output_batch))\n",
    "        #print(\"accuracy_score : \",accuracy_score(batch_tags,output_batch))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #loss = loss_fn(output_batch, labels_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            #output_batch = output_batch.data.cpu().numpy()\n",
    "            #labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        loss1 = loss.data.item()\n",
    "        loss_sum += loss1\n",
    "        \n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.data.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "        batch_len += 1\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    loss_sum /= batch_len\n",
    "    print(\"train loss_sum : \",loss_sum)\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    \n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, metrics, params, num_steps, tag_map_rev):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "    summ_batch_tags = []\n",
    "    summ_batch_pred = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch, batch_pos_w, batch_tags = next(data_iterator)\n",
    "        \n",
    "        summ_batch_tags.extend(batch_tags)\n",
    "        \n",
    "        # compute model output\n",
    "        #output_batch = model.decode(data_batch,batch_pos_w)\n",
    "        loss = model(data_batch,batch_pos_w,labels_batch)\n",
    "        #loss = loss_fn(output_batch, labels_batch)\n",
    "        output_batch = model.forward_decode(data_batch,batch_pos_w,labels_batch)\n",
    "        \n",
    "        summ_batch_pred.extend(output_batch)\n",
    "        #print(loss)\n",
    "        #print(output_batch)\n",
    "        batch_acc = flat_accc(output_batch, labels_batch)\n",
    "        print(\"batch_acc : \",batch_acc)\n",
    "        \n",
    "        #print(\"f1_score : \",f1_score(batch_tags,output_batch))\n",
    "        #print(\"accuracy_score : \",accuracy_score(batch_tags,output_batch))\n",
    "        #print(loss)\n",
    "        #print(output_batch)\n",
    "        #import pdb;pdb.set_trace()\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        #output_batch = output_batch.data.cpu().numpy()\n",
    "        #labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.data.item()\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    summ_batch_pred_n = [[tag_map_rev[each_val] for each_val in val] for val in summ_batch_pred]\n",
    "    summ_batch_tags_n = [[tag_map_rev[each_val] for each_val in val] for val in summ_batch_tags]\n",
    "    #print(\"&&&&&&&&& : \",summ_batch_pred_n[:5])\n",
    "    #print(\"&&&&&&&&& : \",summ_batch_tags_n[:5])\n",
    "    print(\"f1_score : \",f1_score(summ_batch_tags_n,summ_batch_pred_n))\n",
    "    print(\"accuracy_score : \",accuracy_score(summ_batch_tags_n,summ_batch_pred_n))\n",
    "    \n",
    "    return metrics_mean\n",
    "\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, \n",
    "                       data_dir, tag_map_rev, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        dl = data_loader()\n",
    "        train_data_iterator = dl.data_iterator(data_dir, train_data, params, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, metrics, params, num_steps,tag_map_rev)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_data_iterator = dl.data_iterator(data_dir, val_data, params, shuffle=False)\n",
    "        val_metrics = evaluate(model, loss_fn, val_data_iterator, metrics, params, num_steps,tag_map_rev)\n",
    "        print(\"val_metrics : \",val_metrics)\n",
    "        \n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()}, \n",
    "                               is_best=True,\n",
    "                               checkpoint=model_dir)\n",
    "            \n",
    "        # If best_eval, best_save_path        \n",
    "        if is_best:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more util functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_emdd_weights():\n",
    "    # loading vocab (we require this to map words to their indices)\n",
    "    vocab_path = os.path.join(data_dir, 'words.txt')\n",
    "    vocab = {}\n",
    "    with open(vocab_path) as f:\n",
    "        for i, l in enumerate(f.read().splitlines()):\n",
    "            vocab[l] = i\n",
    "\n",
    "    weight = np.zeros((len(vocab),params.embedding_dim))\n",
    "    vocab_inv = {y:x for x,y in vocab.items()}\n",
    "    for ind,val in vocab_inv.items():\n",
    "        try:\n",
    "            weight[ind][:]= emb_model[val]\n",
    "        except:\n",
    "            print(\"exception word not have embeddings : \",val)\n",
    "    return weight\n",
    "\n",
    "def preprocess_postags(pos_list):\n",
    "    '''tr_data = []\n",
    "    for v in tokens_list:\n",
    "        tmp_tr_data = []\n",
    "        for v_in in v:\n",
    "            tmp_tr_data.append(emb_model[v_in.lower()].tolist()) #.astype(np.float64)\n",
    "        tr_data.append(tmp_tr_data)\n",
    "    '''\n",
    "    tr_pos = [onehot_encode(v,unq_pos_list,char_to_int) for v in pos_list]\n",
    "    return tr_pos\n",
    "\n",
    "def preprocess_label(tag_map, labels_list):\n",
    "    fnl_labels = []\n",
    "    for label_sent in labels_list:\n",
    "        tmp_label = []\n",
    "        for label in label_sent:\n",
    "            tmp_label.append(tag_map[label])\n",
    "        fnl_labels.append(tmp_label)\n",
    "    return fnl_labels\n",
    "\n",
    "def get_tags():\n",
    "    tags_path = os.path.join(data_dir, 'tags.txt')\n",
    "    tag_map = {}\n",
    "    with open(tags_path) as f:\n",
    "        for i, t in enumerate(f.read().splitlines()):\n",
    "            tag_map[t] = i\n",
    "    return tag_map,{j:i for i,j in tag_map.items()}\n",
    "\n",
    "def make_equal(x,y):\n",
    "    c = []\n",
    "    d = []\n",
    "    for i,j in zip(x,y):\n",
    "        if len(i) != len(j):\n",
    "            print(\"not equal length : \")\n",
    "            continue\n",
    "        c.append(i)\n",
    "        d.append(j)\n",
    "    return c,d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "/home/adzuser/user_achyuta/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "Starting training for 10 epoch(s)\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception word not have embeddings :  <pad>\n",
      "exception word not have embeddings :  UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s, loss=-6700.436]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.61it/s, loss=-6700.436]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:05,  1.61it/s, loss=-7159.990]\u001b[A\n",
      " 20%|██        | 2/10 [00:02<00:06,  1.17it/s, loss=-7159.990]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:06,  1.17it/s, loss=-8020.655]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.05s/it, loss=-8020.655]\u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:07,  1.05s/it, loss=-8463.609]\u001b[A\n",
      " 40%|████      | 4/10 [00:04<00:07,  1.17s/it, loss=-8463.609]\u001b[A\n",
      " 40%|████      | 4/10 [00:06<00:07,  1.17s/it, loss=-8306.772]\u001b[A\n",
      " 50%|█████     | 5/10 [00:06<00:06,  1.25s/it, loss=-8306.772]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:06,  1.25s/it, loss=-8640.667]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.31s/it, loss=-8640.667]\u001b[A\n",
      " 60%|██████    | 6/10 [00:09<00:05,  1.31s/it, loss=-9208.634]\u001b[A\n",
      " 70%|███████   | 7/10 [00:09<00:04,  1.37s/it, loss=-9208.634]\u001b[A\n",
      " 70%|███████   | 7/10 [00:10<00:04,  1.37s/it, loss=-9326.266]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.39s/it, loss=-9326.266]\u001b[A\n",
      " 80%|████████  | 8/10 [00:12<00:02,  1.39s/it, loss=-9484.282]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.41s/it, loss=-9484.282]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.41s/it, loss=-9580.864]\u001b[A\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.44s/it, loss=-9580.864]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -9580.86376953125\n",
      "batch_acc :  0.0077083333333333335\n",
      "batch_acc :  0.0064285714285714285\n",
      "f1_score :  0.008047044258743424\n",
      "accuracy_score :  0.018054432767448128\n",
      "val_metrics :  {'accuracy': 0.007068452380952381, 'loss': -14817.91064453125}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s, loss=-7013.688]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.49it/s, loss=-7013.688]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:06,  1.49it/s, loss=-7389.520]\u001b[A\n",
      " 20%|██        | 2/10 [00:02<00:07,  1.09it/s, loss=-7389.520]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:07,  1.09it/s, loss=-8181.539]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.08s/it, loss=-8181.539]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:07,  1.08s/it, loss=-8552.420]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.19s/it, loss=-8552.420]\u001b[A\n",
      " 40%|████      | 4/10 [00:06<00:07,  1.19s/it, loss=-8234.160]\u001b[A\n",
      " 50%|█████     | 5/10 [00:06<00:05,  1.14s/it, loss=-8234.160]\u001b[A\n",
      " 50%|█████     | 5/10 [00:06<00:05,  1.14s/it, loss=-8438.785]\u001b[A\n",
      " 60%|██████    | 6/10 [00:06<00:03,  1.12it/s, loss=-8438.785]\u001b[A\n",
      " 60%|██████    | 6/10 [00:06<00:03,  1.12it/s, loss=-8898.631]\u001b[A\n",
      " 70%|███████   | 7/10 [00:06<00:02,  1.39it/s, loss=-8898.631]\u001b[A\n",
      " 70%|███████   | 7/10 [00:06<00:02,  1.39it/s, loss=-8900.015]\u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:01,  1.71it/s, loss=-8900.015]\u001b[A\n",
      " 80%|████████  | 8/10 [00:07<00:01,  1.71it/s, loss=-9039.433]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:07<00:00,  2.05it/s, loss=-9039.433]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:07<00:00,  2.05it/s, loss=-8997.605]\u001b[A\n",
      "100%|██████████| 10/10 [00:07<00:00,  2.37it/s, loss=-8997.605]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -8997.604931640624\n",
      "batch_acc :  0.005\n",
      "batch_acc :  0.002380952380952381\n",
      "f1_score :  0.0026646928201332345\n",
      "accuracy_score :  0.010239827539746699\n",
      "val_metrics :  {'accuracy': 0.0036904761904761906, 'loss': -13347.1396484375}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s, loss=-8804.701]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:04,  2.10it/s, loss=-8804.701]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:04,  2.10it/s, loss=-9734.363]\u001b[A\n",
      " 20%|██        | 2/10 [00:02<00:07,  1.09it/s, loss=-9734.363]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:07,  1.09it/s, loss=-11562.035]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.07s/it, loss=-11562.035]\u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:07,  1.07s/it, loss=-12831.470]\u001b[A\n",
      " 40%|████      | 4/10 [00:04<00:06,  1.05s/it, loss=-12831.470]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:06,  1.05s/it, loss=-12843.249]\u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.05s/it, loss=-12843.249]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:05,  1.05s/it, loss=-13860.599]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.09s/it, loss=-13860.599]\u001b[A\n",
      " 60%|██████    | 6/10 [00:09<00:04,  1.09s/it, loss=-15421.510]\u001b[A\n",
      " 70%|███████   | 7/10 [00:09<00:04,  1.35s/it, loss=-15421.510]\u001b[A\n",
      " 70%|███████   | 7/10 [00:09<00:04,  1.35s/it, loss=-15926.796]\u001b[A\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.05s/it, loss=-15926.796]\u001b[A\n",
      " 80%|████████  | 8/10 [00:11<00:02,  1.05s/it, loss=-16707.422]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:11<00:01,  1.32s/it, loss=-16707.422]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:11<00:01,  1.32s/it, loss=-16996.629]\u001b[A\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.01s/it, loss=-16996.629]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -16996.62861328125\n",
      "batch_acc :  0.007916666666666667\n",
      "batch_acc :  0.005238095238095238\n",
      "f1_score :  0.0042016806722689065\n",
      "accuracy_score :  0.016168148746968473\n",
      "val_metrics :  {'accuracy': 0.006577380952380953, 'loss': -30594.4619140625}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:01<?, ?it/s, loss=-19767.918]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:13,  1.52s/it, loss=-19767.918]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:13,  1.52s/it, loss=-21399.025]\u001b[A\n",
      " 20%|██        | 2/10 [00:02<00:11,  1.48s/it, loss=-21399.025]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:11,  1.48s/it, loss=-24440.780]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.30s/it, loss=-24440.780]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:09,  1.30s/it, loss=-26154.193]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:08,  1.50s/it, loss=-26154.193]\u001b[A\n",
      " 40%|████      | 4/10 [00:06<00:08,  1.50s/it, loss=-25492.070]\u001b[A\n",
      " 50%|█████     | 5/10 [00:06<00:05,  1.14s/it, loss=-25492.070]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:05,  1.14s/it, loss=-26611.765]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.37s/it, loss=-26611.765]\u001b[A\n",
      " 60%|██████    | 6/10 [00:08<00:05,  1.37s/it, loss=-28637.359]\u001b[A\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.07s/it, loss=-28637.359]\u001b[A\n",
      " 70%|███████   | 7/10 [00:10<00:03,  1.07s/it, loss=-28953.905]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.32s/it, loss=-28953.905]\u001b[A\n",
      " 80%|████████  | 8/10 [00:11<00:02,  1.32s/it, loss=-29747.915]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:11<00:01,  1.40s/it, loss=-29747.915]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.40s/it, loss=-29754.015]\u001b[A\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.22s/it, loss=-29754.015]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -29754.015234375\n",
      "batch_acc :  0.009791666666666667\n",
      "batch_acc :  0.006666666666666667\n",
      "f1_score :  0.012752858399296395\n",
      "accuracy_score :  0.019671247642144975\n",
      "val_metrics :  {'accuracy': 0.008229166666666668, 'loss': -45536.62890625}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:01<?, ?it/s, loss=-29447.094]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:11,  1.26s/it, loss=-29447.094]\u001b[A\n",
      " 10%|█         | 1/10 [00:03<00:11,  1.26s/it, loss=-31574.230]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:11,  1.47s/it, loss=-31574.230]\u001b[A\n",
      " 20%|██        | 2/10 [00:05<00:11,  1.47s/it, loss=-35666.565]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:11,  1.58s/it, loss=-35666.565]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:11,  1.58s/it, loss=-37822.499]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.28s/it, loss=-37822.499]\u001b[A\n",
      " 40%|████      | 4/10 [00:07<00:07,  1.28s/it, loss=-36632.998]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:07,  1.48s/it, loss=-36632.998]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:07,  1.48s/it, loss=-37892.393]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.14s/it, loss=-37892.393]\u001b[A\n",
      " 60%|██████    | 6/10 [00:09<00:04,  1.14s/it, loss=-40356.327]\u001b[A\n",
      " 70%|███████   | 7/10 [00:09<00:04,  1.38s/it, loss=-40356.327]\u001b[A\n",
      " 70%|███████   | 7/10 [00:10<00:04,  1.38s/it, loss=-40526.106]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.06s/it, loss=-40526.106]\u001b[A\n",
      " 80%|████████  | 8/10 [00:12<00:02,  1.06s/it, loss=-41331.679]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.32s/it, loss=-41331.679]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.32s/it, loss=-41104.394]\u001b[A\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.02s/it, loss=-41104.394]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -41104.3935546875\n",
      "batch_acc :  0.009583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_acc :  0.0064285714285714285\n",
      "f1_score :  0.012932924156071897\n",
      "accuracy_score :  0.019671247642144975\n",
      "val_metrics :  {'accuracy': 0.008005952380952381, 'loss': -58946.91015625}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:01<?, ?it/s, loss=-38183.195]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:09,  1.10s/it, loss=-38183.195]\u001b[A\n",
      " 10%|█         | 1/10 [00:03<00:09,  1.10s/it, loss=-40744.094]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:10,  1.35s/it, loss=-40744.094]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:10,  1.35s/it, loss=-45771.987]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.05s/it, loss=-45771.987]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:07,  1.05s/it, loss=-48326.969]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.32s/it, loss=-48326.969]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.32s/it, loss=-46675.852]\u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.02s/it, loss=-46675.852]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:05,  1.02s/it, loss=-48074.471]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.29s/it, loss=-48074.471]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.29s/it, loss=-50952.448]\u001b[A\n",
      " 70%|███████   | 7/10 [00:07<00:03,  1.01s/it, loss=-50952.448]\u001b[A\n",
      " 70%|███████   | 7/10 [00:09<00:03,  1.01s/it, loss=-51006.041]\u001b[A\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.29s/it, loss=-51006.041]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.29s/it, loss=-51838.336]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.01s/it, loss=-51838.336]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.01s/it, loss=-51417.152]\u001b[A\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.27s/it, loss=-51417.152]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -51417.1515625\n",
      "batch_acc :  0.009583333333333333\n",
      "batch_acc :  0.0064285714285714285\n",
      "f1_score :  0.012896174863387977\n",
      "accuracy_score :  0.019671247642144975\n",
      "val_metrics :  {'accuracy': 0.008005952380952381, 'loss': -71373.29296875}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:01<?, ?it/s, loss=-46295.000]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.41s/it, loss=-46295.000]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.41s/it, loss=-49273.523]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:08,  1.05s/it, loss=-49273.523]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:08,  1.05s/it, loss=-55183.703]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.35s/it, loss=-55183.703]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:09,  1.35s/it, loss=-58121.781]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:09,  1.54s/it, loss=-58121.781]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:09,  1.54s/it, loss=-56055.533]\u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.17s/it, loss=-56055.533]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:05,  1.17s/it, loss=-57598.062]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.39s/it, loss=-57598.062]\u001b[A\n",
      " 60%|██████    | 6/10 [00:08<00:05,  1.39s/it, loss=-60878.335]\u001b[A\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.08s/it, loss=-60878.335]\u001b[A\n",
      " 70%|███████   | 7/10 [00:10<00:03,  1.08s/it, loss=-60835.520]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.33s/it, loss=-60835.520]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.33s/it, loss=-61703.624]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.03s/it, loss=-61703.624]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.03s/it, loss=-61110.843]\u001b[A\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.29s/it, loss=-61110.843]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -61110.842578125\n",
      "batch_acc :  0.009583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_acc :  0.0064285714285714285\n",
      "f1_score :  0.012859633827375762\n",
      "accuracy_score :  0.019940716787927783\n",
      "val_metrics :  {'accuracy': 0.008005952380952381, 'loss': -83183.47265625}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:01<?, ?it/s, loss=-54024.113]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.40s/it, loss=-54024.113]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.40s/it, loss=-57403.107]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:08,  1.04s/it, loss=-57403.107]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:08,  1.04s/it, loss=-64158.014]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.31s/it, loss=-64158.014]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:09,  1.31s/it, loss=-67465.821]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:08,  1.50s/it, loss=-67465.821]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:08,  1.50s/it, loss=-65009.176]\u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.19s/it, loss=-65009.176]\u001b[A\n",
      " 50%|█████     | 5/10 [00:06<00:05,  1.19s/it, loss=-66694.201]\u001b[A\n",
      " 60%|██████    | 6/10 [00:06<00:03,  1.12it/s, loss=-66694.201]\u001b[A\n",
      " 60%|██████    | 6/10 [00:08<00:03,  1.12it/s, loss=-70365.099]\u001b[A\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.21s/it, loss=-70365.099]\u001b[A\n",
      " 70%|███████   | 7/10 [00:09<00:03,  1.21s/it, loss=-70235.721]\u001b[A\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.16s/it, loss=-70235.721]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.16s/it, loss=-71143.734]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.20s/it, loss=-71143.734]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.20s/it, loss=-70390.789]\u001b[A\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.11it/s, loss=-70390.789]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -70390.789453125\n",
      "batch_acc :  0.009583333333333333\n",
      "batch_acc :  0.006666666666666667\n",
      "f1_score :  0.012858232537866405\n",
      "accuracy_score :  0.02021018593371059\n",
      "val_metrics :  {'accuracy': 0.008125, 'loss': -94542.0078125}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s, loss=-61469.500]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:08,  1.11it/s, loss=-61469.500]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:08,  1.11it/s, loss=-65235.617]\u001b[A\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.21s/it, loss=-65235.617]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:09,  1.21s/it, loss=-72807.318]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:06,  1.05it/s, loss=-72807.318]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:06,  1.05it/s, loss=-76474.076]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.25s/it, loss=-76474.076]\u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.25s/it, loss=-73643.280]\u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:04,  1.03it/s, loss=-73643.280]\u001b[A\n",
      " 50%|█████     | 5/10 [00:07<00:04,  1.03it/s, loss=-75468.040]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.25s/it, loss=-75468.040]\u001b[A\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.25s/it, loss=-79518.920]\u001b[A\n",
      " 70%|███████   | 7/10 [00:07<00:02,  1.01it/s, loss=-79518.920]\u001b[A\n",
      " 70%|███████   | 7/10 [00:09<00:02,  1.01it/s, loss=-79309.318]\u001b[A\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.26s/it, loss=-79309.318]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.26s/it, loss=-80258.837]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.19s/it, loss=-80258.837]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:11<00:01,  1.19s/it, loss=-79353.850]\u001b[A\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.06s/it, loss=-79353.850]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -79353.85\n",
      "batch_acc :  0.011041666666666667\n",
      "batch_acc :  0.010238095238095239\n",
      "f1_score :  0.014675731088809756\n",
      "accuracy_score :  0.026138507140932365\n",
      "val_metrics :  {'accuracy': 0.010639880952380953, 'loss': -105546.78125}\n",
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s, loss=-68695.453]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:02,  3.79it/s, loss=-68695.453]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:02,  3.79it/s, loss=-72837.496]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:02,  3.84it/s, loss=-72837.496]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:02,  3.84it/s, loss=-81204.523]\u001b[A\n",
      " 30%|███       | 3/10 [00:00<00:01,  3.70it/s, loss=-81204.523]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:01,  3.70it/s, loss=-85222.752]\u001b[A\n",
      " 40%|████      | 4/10 [00:01<00:01,  3.53it/s, loss=-85222.752]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:01,  3.53it/s, loss=-82032.116]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  2.11it/s, loss=-82032.116]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:02,  2.11it/s, loss=-83997.289]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:03,  1.09it/s, loss=-83997.289]\u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:03,  1.09it/s, loss=-88423.324]\u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:03,  1.20s/it, loss=-88423.324]\u001b[A\n",
      " 70%|███████   | 7/10 [00:06<00:03,  1.20s/it, loss=-88140.898]\u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:02,  1.01s/it, loss=-88140.898]\u001b[A\n",
      " 80%|████████  | 8/10 [00:08<00:02,  1.01s/it, loss=-89135.665]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:08<00:01,  1.29s/it, loss=-89135.665]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:08<00:01,  1.29s/it, loss=-88087.219]\u001b[A\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.01s/it, loss=-88087.219]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss_sum :  -88087.21875\n",
      "batch_acc :  0.01375\n",
      "batch_acc :  0.011666666666666667\n",
      "f1_score :  0.014675731088809756\n",
      "accuracy_score :  0.03233629749393695\n",
      "val_metrics :  {'accuracy': 0.012708333333333334, 'loss': -116340.4296875}\n",
      "Checkpoint Directory exists! \n"
     ]
    }
   ],
   "source": [
    "restore_file=None\n",
    "\n",
    "# Load the parameters from json file\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "    \n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "        \n",
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "params.vocab_size=sizes['vocab_size']\n",
    "params.number_of_tags = sizes['number_of_tags']\n",
    "\n",
    "# convert pos-tags in data to one-hot vector.\n",
    "tr_pos = None\n",
    "ts_pos = None\n",
    "tr_pos = preprocess_postags(tr_pos_list)\n",
    "ts_pos = preprocess_postags(ts_pos_list)\n",
    "\n",
    "# loading tags (we require this to map tags to their indices)\n",
    "tag_map,tag_map_rev = get_tags()\n",
    "\n",
    "# convert labels/tags to index.\n",
    "tr_label_list = preprocess_label(tag_map, tr_labels_list)\n",
    "ts_label_list = preprocess_label(tag_map, ts_labels_list)\n",
    "\n",
    "# select if tokens and labels size equal.\n",
    "tr_tokens_list_1, tr_label_list_1 = make_equal(tr_tokens_list,tr_label_list)\n",
    "ts_tokens_list_1, ts_label_list_1 = make_equal(ts_tokens_list,ts_label_list)\n",
    "\n",
    "# short it for faster buildings. can comment below section for including all data.\n",
    "tr_tokens_list_1 = tr_tokens_list_1[:500]\n",
    "tr_pos = tr_pos[:500]\n",
    "tr_label_list_1 = tr_label_list_1[:500]\n",
    "ts_tokens_list_1 = ts_tokens_list_1[:100]\n",
    "ts_pos = ts_pos[:100]\n",
    "ts_label_list_1 = ts_label_list_1[:100]\n",
    "\n",
    "train_data = {'data':tr_tokens_list_1,'pos':tr_pos,'labels':tr_label_list_1,'size':len(tr_tokens_list_1)}\n",
    "val_data = {'data':ts_tokens_list_1,'pos':ts_pos,'labels':ts_label_list_1,'size':len(ts_tokens_list_1)}\n",
    "\n",
    "# specify the train and val dataset sizes\n",
    "params.train_size = train_data['size']\n",
    "params.val_size = val_data['size']\n",
    "\n",
    "# get weight matrix for each vocab to initialize model with pre-trained embeddings.\n",
    "weight = get_word_emdd_weights()\n",
    "\n",
    "# Define the model and optimizer\n",
    "model = net.Netcp(params).cuda() if params.cuda else net.Net(params)\n",
    "model.init_word_embeddings(torch.FloatTensor(weight).cuda())\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    \n",
    "# fetch loss function and metrics\n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, data_dir, tag_map_rev,\n",
    "                    restore_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_acc :  0.01375\n",
      "batch_acc :  0.011666666666666667\n",
      "f1_score :  0.014675731088809756\n",
      "accuracy_score :  0.03233629749393695\n",
      "{'accuracy': 0.012708333333333334, 'loss': -116340.4296875}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "restore_file='best'\n",
    "\n",
    "# Load the parameters\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()     # use GPU is available\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "params.vocab_size=sizes['vocab_size']\n",
    "params.number_of_tags = sizes['number_of_tags']\n",
    "\n",
    "# load data\n",
    "#data_loader = DataLoader(data_dir, params)\n",
    "#data = data_loader.load_data(['test'], data_dir)\n",
    "#test_data = data['test']\n",
    "\n",
    "test_data = val_data\n",
    "\n",
    "# specify the test set size\n",
    "params.test_size = test_data['size']\n",
    "#test_data_iterator = data_loader.data_iterator(test_data, params)\n",
    "dll = data_loader()\n",
    "test_data_iterator = dll.data_iterator(data_dir, test_data, params)\n",
    "\n",
    "# Define the model\n",
    "model = net.Netcp(params).cuda() if params.cuda else net.Net(params)\n",
    "    \n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "# Reload weights from the saved file\n",
    "utils.load_checkpoint(os.path.join(model_dir, restore_file + '.pth.tar'), model)\n",
    "\n",
    "# Evaluate\n",
    "num_steps = (params.test_size + 1) // params.batch_size\n",
    "test_metrics = evaluate(model, loss_fn, test_data_iterator, metrics, params, num_steps, tag_map_rev)\n",
    "save_path = os.path.join(model_dir, \"metrics_test_{}.json\".format(restore_file))\n",
    "#utils.save_dict_to_json(test_metrics, save_path)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
